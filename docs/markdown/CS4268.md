---
title: CS4268 Quantum Computing
---

#### Linear Alg
* Shorthands: i = Row index. j = Col index.
* [Invertible](https://en.wikipedia.org/wiki/Invertible_matrix): $A(A^{-1})$ = $(A^{-1})A$ = $I_n$. Only square matrices can be invertible
  * Inverse is unique
  * Nonsingular: Determinant $\neq$ 0
  * Prata: $(AB)^{-1} = B^{-1}A^{-1}$; likewise, $(AB)^{*} = B^{*}A^{*}$
  * No mercy to lonely: $(kA)^{-1} = k^{-1}A^{-1}$
  * 360 no-scope: $(A^-1)^{-1} = A$
  * Transpose's *kyōdai*: $(A^T)^{-1} = (A^{-1})^{T}$
* [Transpose](https://en.wikipedia.org/wiki/Transpose)
  * Addition: $(A+B)^T = A^T + B^T$ (no diff)
  * Prata: $(AB)^{T} = B^{T}A^{T}$
  * Spares the lonely: $(kA)^{T} = kA^{T}$
  * 360 no-scope: $(A^T)^{T} = A$
  * Invert's *kyōdai*: $(A^T)^{-1} = (A^{-1})^{T}$
* Trace of square matrix M Tr(M): sum of its diagonal elements.
* Dot product: $row\cdot col = \sum_{i=1}^{n}row_icol_i$
  * For 2 vectors, Inner product is the (scalar) result of the dot product
  * For 2 matrices, Inner product is $<A,B> = Tr(A^{\dagger} \cdot B}$
* [Matrix Multiplication](https://math.stackexchange.com/questions/2063241/matrix-multiplication-notation): $c_{ij} = \sum_{k}^n = a_{ik}b_{kj}$
* $||v||_1$ = 1-norm = $\max_j(\sum_{i=1}^n|a_{ij}|)$
* $||v||_2$ = 2-norm / Euclidean norm = $\sqrt{\sum_{i=1}^n\sum_{j=1}^n|a_{ij}|^2}$
* Complex number: $a + bi, i=\sqrt{-1}, i^2=-1$
  * [Conjugate](https://en.wikipedia.org/wiki/Complex_conjugate): $a - bi$
    * Distributive over +,-,x,$\div$
    * $zz^*=|z|^2$, $z^{-1}=\frac{z^*}{|z|^2}, \forall z \neq 0$
    * Commutative (changing order doesn't change result) over power, exponent or if z (non-zero) is logged
      * $(z^n)^* = (z^*)^n$
      * $exp(z^*) = e^{z^*} = (e^z)^*$
      * $log(z^*) = (log(z))^*$
  * Dot product: $v\cdot u = v^{\dagger}u$
* Singular Value Decomposition (SVD): Factorization/Decomposition (operation to break matrix into product of multiple matrices).
  * Always exists for any matrix.
  * $M = U\sum V^*$:
    * U: m x m complex unitary matrix
    * V: n x n complex unitary matrix
    * $\sum$: m x n rectangular diagonal matrix
    * M: m x n complex matrix. If real, U and $V^T = V^*$ are orthogonal matrices
    * Compact SVD: similar procedure, but $\sum$ is square diagonal r x r. 
    * U: m x r complex unitary matrix
    * V: r x n complex unitary matrix
    * $U^*U = V^*V = I_{rxr}$
    * $\sum$: r x r rectangular diagonal matrix
    * M: m x n complex matrix. If real, U and $V^T = V^*$ are orthogonal matrices
* Orthogonal Matrix iff transpose = inverse ($A^T = A^{-1}$)
  * $A^{-1}$: [Inverse is also orthogonal](https://proofwiki.org/wiki/Inverse_of_Orthogonal_Matrix_is_Orthogonal)
  * Columns form an orthogonal set of vectors.
  * [Rows form an orthogonal set of vectors.](http://www.math.lsa.umich.edu/~kesmith/OrthogonalTrans2.pdf)
  * Multiplication with Tranpose is same as inverse: $A^TA = I = AA^T$
* [Unitary matrix](https://en.wikipedia.org/wiki/Unitary_matrix) U:
  * $U^*U = I_{nxn} = UU^*$ (Tut1)
  * $U^{\dagger}U = I = UU^{\dagger}$ (Tut1)
  * For any vector $v, ||U\cdot v||_2 = ||v||_2$ (Tut 1)
    * $U^{\dagger} = (U^*)^T$
  * Columns of U form an orthonormal set (Tut1)
  * Rows of U form an orthonormal set (Tut1)
  * Product of 2 unitary matrices are always unitary ([Proof](https://www.quora.com/Is-the-product-of-two-unitary-matrices-always-unitary))
  * These rules also apply to the tranpose of U ($U^T$): ([Proof](https://math.stackexchange.com/questions/1242317/when-is-the-transpose-of-a-square-unitary-matrix-also-unitary))
  * and the conjugate of U ($U^*$): ($U^*(U^*)^{\dagger}=U^*(U^T)=(U^\dagger)^TU^T=(UU^\dagger)^T=I^T=I$)
#### Classical States
$v=(\begin{array}{l} a\\ b\end{array}) \equiv (\begin{array}{l} Pr(X=0)\\ Pr(X=1)\end{array})$
* This "Pr(X=0)" is affected by your belief (i.e. if X=0 after measurement, then v = [1,0]^T)

Stochastic Matrices 
  * **Properties**
    * $||v||_1 = v[0] + v[1] = 1$, Preserved by stochastic matrices
    * Left stochastic matrix: All columns sum to 1
      * Right stochastic matrix: All rows sum to 1
      * Doubly stochastic matrix: Any direction sum to 1
    * All entries are non negative
  * **Operations**: Av = u, where v is probability vector (specified above), and A is:
    * $\begin{array}{ll} 
    \operatorname{INIT}_{0} & =\left(\begin{array}{ll}1 & 1 \\ 0 & 0\end{array}\right)(\begin{array}{l} a\\ b\end{array}) = (\begin{array}{l} 1\\ 0\end{array})\\\hline
    \operatorname{INIT}_{1} & =\left(\begin{array}{ll}0 & 0 \\ 1 & 1\end{array}\right)(\begin{array}{l} a\\ b\end{array}) = (\begin{array}{l} 0\\ 1\end{array})\\\hline
    NOT & =\left(\begin{array}{ll}0 & 1 \\ 1 & 0\end{array}\right)(\begin{array}{l} a\\ b\end{array}) = (\begin{array}{l} b\\ a\end{array})\\\hline
    I & =\left(\begin{array}{ll}1 & 0 \\ 0 & 1\end{array}\right)(\begin{array}{l} a\\ b\end{array}) = (\begin{array}{l} a\\ b\end{array})
    \end{array}$


#### Qubit (quantum state / superposition)
* $v=(\begin{array}{l} \alpha\\ \beta\end{array}) \equiv (\begin{array}{l} Pr(X=0)\\ Pr(X=1)\end{array})$
  * $\alpha, \beta$ can be imaginary ($a + ib$, where $a,b\in R$ and $i=\sqrt{-1}$)
  * **Norm2 / Euclidean norm:** $||\alpha||_2 = \sqrt{a_{\alpha}^2 + b_{\alpha}^2}$
* **Properties:**
  * Main prop: $A^+A = AA^+ = I$, where $A^+$ = conjugate transpose (also known as [adjoint of A](https://mathworld.wolfram.com/Adjoint.html))
  * $||v||_2 = ||v[0]||^2 + ||v[1]||^2 = 1$, Preserved by unitary matrices
  * Norm2 of all columns sum to 1
  * Orthonormal columns and rows (dot product of every pair of vectors in matrix = 0)
    * $A^*$: Conjugate; Convert $(a+ib)$ to $(a-ib)$ $\forall$ entries
    * $A^T$: Transpose; Rows (Left to Right) become Columns (Up to Down). Iterate from Up to Down, Stack from Left to Right
  * Conjugate transpose is distributive: $(A+B)^+ = A^+ + B^+$, $(AB)^+ = B^+ A^+$
* **Operations**:
  * $H=\left(\begin{array}{cc}\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}\end{array}\right), \quad I=\left(\begin{array}{cc}1 & 0 \\ 0 & 1\end{array}\right), \quad \mathrm{NOT}=\left(\begin{array}{cc}0 & 1 \\ 1 & 0\end{array}\right), \quad R_{\theta}=\left(\begin{array}{cc}\cos (\theta) & -\sin (\theta) \\ \sin (\theta) & \cos (\theta)\end{array}\right)$
    * H: Hadamard, R: rotation
    * Measure unknown Qubit vectors with Hadamard to get its 0,1/1,0 value

#### Tensor Product
* A ⊗ B: $\forall$ entry $a_{ij}$ in A, Multiply it with B ($a_{ij}B$), then slot that matrix into $a_{ij}$'s position.
* [Properties](https://en.wikipedia.org/wiki/Kronecker_product): [See also](https://mathworld.wolfram.com/KroneckerProduct.html) 
  * Associative: A ⊗ (B ⊗ C) = A ⊗ B ⊗ C
  * Mixed Product property: (A ⊗ B)(C ⊗ D) = (AC)⊗(BD)
  * Distributive over addition: 
    * A ⊗ (B+C) = A⊗B + A⊗C
    * (A+B)⊗C = A⊗C + B⊗C
  * Scalars float freely (can be complex):
    * (aA)⊗B = A⊗(aB) = a(A⊗B)
* Concepts:
  * Independent RV / Independent System: if a multi-bit vector can be written as a tensor product
  * Correlated RV / Entangled System: if a multi-bit vector cannot be written as a tensor product

#### Dirac-Notation
* $\langle A| \doteq\left(\begin{array}{llll}A_{1}^{*} & A_{2}^{*} & \cdots & A_{N}^{*}\end{array}\right)$
* $|B\rangle \doteq\left(\begin{array}{c}B_{1} \\ B_{2} \\ \vdots \\ B_{N}\end{array}\right)$
* |0> ("ket-0") = $(\begin{array}{l} 1\\ 0\end{array})$, |1> ("ket-1") = $(\begin{array}{l} 0\\ 1\end{array})$
* Scalar x ket = multiplication
* A ket next to a ket is a tensor product: |00> = |0>|0> = |0>⊗|0>
  * Value in ket = represents a 1 in the (bin-value + 1)th bit (1st bit: |00>, 3rd bit: |10>)
  * Multibit: Shifting. |1>|0> = |10>. (|0> - |1>)⊗|1> = (|01> - |11>)
* Definitions:
  * |$\psi$> = $\frac{1}{\sqrt{2}}$(|00> + |11>) = $\frac{1}{\sqrt{2}}$(|0>|0> + |1>|1>)
  * I|any> = |any> 
  * H|0> = $\frac{1}{\sqrt{2}}$(|0> + |1>), H|1> = $\frac{1}{\sqrt{2}}$(|0> - |1>)
* $\left\langle\left. A\right|^{\dagger}=\mid A\right\rangle, \quad|A\rangle^{\dagger}=\langle A|$
* < x| ("bra") = $(|x>)^+$ (conjugate-transpose)
  * A bra next to a ket is a matrix multiplication.
  * < x|y > = bra-ket = row x col = inner-product (dot product)
  * |x> < y| = ket-bra = col x row = outer-product (small side multiplication)
  * Recall ket is a col vector. You can exploit its associative prop: (|a> < b|)|c> = |a>< b|c>