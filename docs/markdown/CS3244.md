---
title: CS3244 Machine Learning
---

* [Collaborative & AutoML](https://www.comp.nus.edu.sg/~lowkh/research.html)
  * Trusted data sharing, tweaking params
* Learning Element
  * Which components are to be learnt
  * How to represent the data & the components
  * What feedback is available
  * Main goal of the model
  * Supervised learning: answer is provided
    * Learning an unknown function (focus of the module) that best fits answer  
  * Unsupervised learning: no answer provided
  * Reinforcement learning: rewards provided

* Concept Learning (Simplified ML)
  * Concept: boolean-value f(x) we want to learn
  * Infer unknown boolean value function
  * Training sample: row
    * Input Instance (X): comprises of Input Attributes (Features)
      * Input Attributes: Categorical variables
    * Boolean Output Attribute (Target): Y/N, 1/0, +ve/-ve
  * Hypothesis: Conjunction of constraints on input attr
    * Think of this as a firewall filter
    * For every Input attr: X = specified_categorical_val, X=? (don't care), X=null
    * Input Instance satisfies (all constraints of) hypothesis if h(x) = 1
    * Expressive power (scope) vs hypothesis space (search space)
      * e.g. y = mx + c vs y = ax^2 + mx + c
      * More expressive model: more time & data to train
    * Hypothesis Space: Set of hypothesis 
  * Goal: Find hypothesis that is consistent with noise-free training sample. 
    * $\forall$ training instances, h(x) = 1 iff +ve instance, h(x) = 0 iff -ve instance
    * Syntactically distinct (exact # of permutations)
    * Semantically distinct: don't cover the same subset of input instances (i.e. if none of the instances have a null value, then you can group any hypothesis with null filter as 1)
    * Exploit Structure: (h1(x)=1)->(h2(x)=1)
      * More general or equal: $\geq_g$, $\gt_g$
      * Partial order: reflexive, antisymmetric, transitive
      * **Find-s algorithm**:
        * Start with most specific hypothesis
        * When it wrongly classifies 1 as 0, minimally generalize it to satisfy input instance.
        * Resultant hypothesis is a hypothesis in the **Specific Boundary** (summary of +ve training examples).
          * As long as the hypothesis space contains a hypothesis that describes the true target concept,
and the training data contains no errors, ignoring negative examples does not cause to any
problem.
          * Limitations:
            * Can't tell if Find-S found the target concept
            * Can't tell if training examples are noisy
            * Picks a maximally specific hypothesis (may not be unique)
* Version Space w.r.t. hyp. space H and training examples D ($VS_{H,D}$)
  * Subset of hypotheses from H that are consistent with D
  * A large enough D can reduce $VS_{H,D}$ to just 1 hypothesis
* **List-then-eliminate**
  * Init as H, then reduce it to $VS_{H,D}$ by enumerating D, removing $h$ where $h(x)\neq c(x)$. This results in the **general boundary**.
  * Limitation: expensive to enumerate
* Version space representation theorem (VSRT):
  * **Specific boundary S**: set of most specific members of H consistent with D. (summary of +ve training examples; s(x)=1 -> h(x)=1)
  * **General Boundary G**: Set of the most general members of H consistent with D. (summary of -ve training examples; ; g(x)=0 -> h(x)=0)
  * Every consistent hypothesis lies on the path between S and G (inclusive) (S19, showing all consistent hypothesis)
* **Candidate-elimination algorithm**
  * G (most general set) = {(all wildcard)}, S (most specific set)= {(all null)}.
  * +ve Training example d:
    * If a hypothesis in S doesn't satisfy d, remove it, then add its least generalized versions (most attr changed, but minimal wildcards) h s.t.:
      * h satisfies d
      * h is less general than at least 1 h in G
    * Remove any hypothesis from G that doesn't satisfy d
  * -ve Training example d:
    * Remove any hypothesis from S that satisfies d
    * If a hypothesis in G satisfies d, remove it, then add every least specialized hypothesis (fewest attributes changed) s.t.:
      * h doesn't satisfy d
      * h is more general than at least 1 h in S
  * Limitations:
    * Noisy data (S & G will both fit to null)
    * Not enough data (insufficiently expressive hypothesis expression)
  * Active Learner: should try to actively halve the search space; requires at least $log_2(VS_{H,D})$ examples to find target concept
  
* Inductive bias assumpion
  * Hypothesis found from sufficiently large training set also applies to unobserved data
* Summarize data: save time & space