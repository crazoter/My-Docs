---
title: CS3244 Machine Learning
---

* [Collaborative & AutoML](https://www.comp.nus.edu.sg/~lowkh/research.html)
  * Trusted data sharing, tweaking params
* Learning Element
  * Which components are to be learnt
  * How to represent the data & the components
  * What feedback is available
  * Main goal of the model
  * Supervised learning: answer is provided
    * Learning an unknown function (focus of the module) that best fits answer  
  * Unsupervised learning: no answer provided
  * Reinforcement learning: rewards provided

* Concept Learning (Simplified ML)
  * Concept: boolean-value f(x) we want to learn
  * Infer unknown boolean value function
  * Training sample: row
    * Input Instance (X): comprises of Input Attributes (Features)
      * Input Attributes: Categorical variables
    * Boolean Output Attribute (Target): Y/N, 1/0, +ve/-ve
  * Hypothesis: Conjunction of constraints on input attr
    * Think of this as a firewall filter
    * For every Input attr: X = specified_categorical_val, X=? (don't care), X=null
    * Input Instance satisfies (all constraints of) hypothesis if h(x) = 1
    * Expressive power (scope) vs hypothesis space (search space)
      * e.g. y = mx + c vs y = ax^2 + mx + c
      * More expressive model: more time & data to train
    * Hypothesis Space: Set of hypothesis 
  * Goal: Find hypothesis that is consistent with noise-free training sample. 
    * $\forall$ training instances, h(x) = 1 iff +ve instance, h(x) = 0 iff -ve instance
    * Syntactically distinct (exact # of permutations)
    * Semantically distinct: don't cover the same subset of input instances (i.e. if none of the instances have a null value, then you can group any hypothesis with null filter as 1)
    * Exploit Structure: (h1(x)=1)->(h2(x)=1)
      * More general or equal: $\geq_g$, $\gt_g$
      * Partial order: reflexive, antisymmetric, transitive
      * Find-s algorithm:
        * Start with most specific hypothesis
        * When it wrongly classifies 1 as 0, minimally generalize it to satisfy input instance
        * 
* Inductive bias assumpion
  * Hypothesis found from sufficiently large training set also applies to unobserved data
  * 