<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.66">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous"><title data-react-helmet="true">AI Planning (Reinforcement Learning) | Matt&#x27;s Docs</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="docusaurus_language" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="AI Planning (Reinforcement Learning) | Matt&#x27;s Docs"><meta data-react-helmet="true" name="description" content="Bellman Equation"><meta data-react-helmet="true" property="og:description" content="Bellman Equation"><meta data-react-helmet="true" property="og:url" content="https://crazoter.github.io/My-Docs/docs/markdown/ai_planning"><link data-react-helmet="true" rel="shortcut icon" href="/My-Docs/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://crazoter.github.io/My-Docs/docs/markdown/ai_planning"><link rel="stylesheet" href="/My-Docs/styles.0ba35bfa.css">
<link rel="preload" href="/My-Docs/styles.2c9d9d1b.js" as="script">
<link rel="preload" href="/My-Docs/runtime~main.9dbf4ab6.js" as="script">
<link rel="preload" href="/My-Docs/main.37720434.js" as="script">
<link rel="preload" href="/My-Docs/common.33db9307.js" as="script">
<link rel="preload" href="/My-Docs/2.6eb0d358.js" as="script">
<link rel="preload" href="/My-Docs/28.f7ca7a2d.js" as="script">
<link rel="preload" href="/My-Docs/29.d9316f6b.js" as="script">
<link rel="preload" href="/My-Docs/935f2afb.3211b5cf.js" as="script">
<link rel="preload" href="/My-Docs/17896441.fdf12432.js" as="script">
<link rel="preload" href="/My-Docs/4910ebf8.77fdcaa0.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/My-Docs/"><img class="navbar__logo" src="/My-Docs/img/ibuki.png" alt="My Site Logo"><strong class="navbar__title">Matt&#x27;s Docs</strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/My-Docs/docs/">Docs</a><a class="navbar__item navbar__link" href="/My-Docs/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/crazoter/My-Docs" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub</a><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">üåú</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">üåû</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/My-Docs/"><img class="navbar__logo" src="/My-Docs/img/ibuki.png" alt="My Site Logo"><strong class="navbar__title">Matt&#x27;s Docs</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/My-Docs/docs/">Docs</a></li><li class="menu__list-item"><a class="menu__link" href="/My-Docs/blog">Blog</a></li><li class="menu__list-item"><a href="https://github.com/crazoter/My-Docs" target="_blank" rel="noopener noreferrer" class="menu__link">GitHub</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Docusaurus</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/My-Docs/docs/docusaurus/usage">Usage</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/My-Docs/docs/docusaurus/doc1">Style Guide</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/My-Docs/docs/docusaurus/mdx">Powered by MDX</a></li></ul></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">AI</a><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/My-Docs/docs/markdown/ai_planning">AI Planning (Reinforcement Learning)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/My-Docs/docs/markdown/machine_learning">Machine Learning</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Languages</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/My-Docs/docs/markdown/python">Python Cheatsheet</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Math</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/My-Docs/docs/markdown/math">Math</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Networks</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/My-Docs/docs/markdown/network">Network</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Software Development</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/My-Docs/docs/markdown/formal_verification">Formal Verification</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Misc</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/My-Docs/docs/">Technologies you should know</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><header><h1 class="docTitle_1Lrw">AI Planning (Reinforcement Learning)</h1></header><div class="markdown"><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="bellman-equation"></a>Bellman Equation<a aria-hidden="true" tabindex="-1" class="hash-link" href="#bellman-equation" title="Direct link to heading">#</a></h1><ul><li>Apply concept of dynamic programming and optimal substructure to do <strong>policy evaluation</strong></li></ul><p><strong>R(s):</strong>
<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mi>R</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>+</mo><mi>Œ≥</mi><msub><mo><mi>max</mi><mo>‚Å°</mo></mo><mrow><mi>a</mi><mo>‚àà</mo><mi>A</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">(</mo><msub><mo>‚àë</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup></msub><mi>P</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup><mi mathvariant="normal">‚à£</mi><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mi>V</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(s) = R(s) + Œ≥ \max_{a ‚àà A(s)}( \sum_{s&#x27;} P(s&#x27;|s,a)V(s&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.107092em;vertical-align:-0.3551999999999999em"></span><span class="mord mathnormal" style="margin-right:0.05556em">Œ≥</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em"><span style="top:-2.5198em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mrel mtight">‚àà</span><span class="mord mathnormal mtight">A</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em">‚àë</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.17826999999999993em"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828285714285715em"><span style="top:-2.786em;margin-right:0.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">‚Ä≤</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚Ä≤</span></span></span></span></span></span></span></span></span><span class="mord">‚à£</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚Ä≤</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><ul><li><strong>Value of a state</strong> = <strong>its value</strong> + (Exploration Factor) x <strong>best value of next state</strong><ul><li>How to get best value of next state?<ul><li>By choosing best action: the action that maximizes expected next state value<ul><li>Given (s, a), an action has the value of the sum of (value of next states) x (probability of reaching them).</li></ul></li></ul></li></ul></li></ul><table><thead><tr><th>Var</th><th>Descript</th></tr></thead><tbody><tr><td>s</td><td>Any possible state</td></tr><tr><td>s&#x27;</td><td>Any possible next state from s</td></tr><tr><td>a</td><td>Any possible action at s</td></tr><tr><td>V(s)</td><td>Value of s</td></tr><tr><td>R(s)</td><td>Reward of s (for staying on it)</td></tr><tr><td>A(s)</td><td>All possible actions at state s</td></tr><tr><td>Œ≥</td><td>Exploration Factor</td></tr><tr><td>P(s&#x27;|s,a)</td><td>Probability of s&#x27; given (s, a)</td></tr></tbody></table><p><strong>Bellman Equation Variants</strong></p><p><strong>R(s,a,s&#x27;):</strong>
<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mo><mi>max</mi><mo>‚Å°</mo></mo><mrow><mi>a</mi><mo>‚àà</mo><mi>A</mi></mrow></msub><msub><mo>‚àë</mo><mrow><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup><mo>‚àà</mo><mi>S</mi></mrow></msub><mi>T</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">[</mo><mi>R</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup><mo stretchy="false">)</mo><mo>+</mo><mi>Œ≥</mi><mi>V</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">V(s) = \max_{a\in A} \sum_{s&#x27;\in S} T(s,a,s&#x27;)[R(s,a,s&#x27;) + \gamma V(s&#x27;)]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.078972em;vertical-align:-0.32708000000000004em"></span><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em"><span style="top:-2.5500000000000003em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mrel mtight">‚àà</span><span class="mord mathnormal mtight">A</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.17737em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em">‚àë</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.17862099999999992em"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828285714285715em"><span style="top:-2.786em;margin-right:0.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">‚Ä≤</span></span></span></span></span></span></span></span></span><span class="mrel mtight">‚àà</span><span class="mord mathnormal mtight" style="margin-right:0.05764em">S</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.32708000000000004em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚Ä≤</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚Ä≤</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.001892em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.05556em">Œ≥</span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚Ä≤</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">]</span></span></span></span></span></p><ul><li>Reward for transiting from s to s&#x27; via a</li><li>Choose the action that has gives the highest (transition reward + discounted future value of that next state)</li></ul><p><strong>R(s,a):</strong>
<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mo><mi>max</mi><mo>‚Å°</mo></mo><mrow><mi>a</mi><mo>‚àà</mo><mi>A</mi></mrow></msub><mi>R</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>+</mo><mi>Œ≥</mi><msub><mo>‚àë</mo><mrow><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup><mo>‚àà</mo><mi>S</mi></mrow></msub><mi>T</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup><mo stretchy="false">)</mo><mi>V</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(s) = \max_{a\in A}R(s,a) + \gamma \sum_{s&#x27;\in S} T(s,a,s&#x27;)V(s&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em"><span style="top:-2.5500000000000003em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mrel mtight">‚àà</span><span class="mord mathnormal mtight">A</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.17737em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.078972em;vertical-align:-0.32708000000000004em"></span><span class="mord mathnormal" style="margin-right:0.05556em">Œ≥</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em">‚àë</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.17862099999999992em"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828285714285715em"><span style="top:-2.786em;margin-right:0.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">‚Ä≤</span></span></span></span></span></span></span></span></span><span class="mrel mtight">‚àà</span><span class="mord mathnormal mtight" style="margin-right:0.05764em">S</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.32708000000000004em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚Ä≤</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚Ä≤</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><ul><li>Reward for taking action a at state s</li><li>Choose the action that has gives the (highest reward + discounted value of all possible next states using said action)</li></ul><p><strong>Converting between reward functions</strong></p><p><strong>R(s,a,s&#x27;) -&gt; R(s,a)</strong>:
<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>R</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mo>‚àë</mo><mrow><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup><mo>‚àà</mo><mi>S</mi></mrow></msub><mi>T</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup><mo stretchy="false">)</mo><mi>R</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">R&#x27;(s,a) = \sum_{s&#x27;\in S}T(s,a,s&#x27;)R(s,a,s&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.001892em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚Ä≤</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.078972em;vertical-align:-0.32708000000000004em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em">‚àë</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.17862099999999992em"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828285714285715em"><span style="top:-2.786em;margin-right:0.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">‚Ä≤</span></span></span></span></span></span></span></span></span><span class="mrel mtight">‚àà</span><span class="mord mathnormal mtight" style="margin-right:0.05764em">S</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.32708000000000004em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚Ä≤</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚Ä≤</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><ul><li>The reward for taking an action is the sum of (all its possible transitions) x (related transition rewards)</li></ul><p><strong>R(s,a) -&gt; R(s)</strong>:
R&#x27;(post(s,a)) = \gamma^{-1/2}R(s,a)</p><ul><li>s&#x27; is abstracted out to &quot;post(s,a)&quot;, which refers to the &quot;post-state&quot; for every (s,a). In this R:</li></ul><div class="mdxCodeBlock_1XEh"><div class="codeBlockContent_1u-d"><button tabindex="0" type="button" aria-label="Copy code to clipboard" class="copyButton_10dd">Copy</button><div class="prism-code language-undefined codeBlock_3iAC"><div class="codeBlockLines_b7E3" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">// (s,a) always goes to post(s,a)</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">T&#x27;(s,a,post(s,a)) = 1</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">// The probability is factored in here</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">T&#x27;(post(s,a),b,s&#x27;) = T(s,a,s&#x27;)</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">// The states themselves don&#x27;t have rewards</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">R&#x27;(s) = 0</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">// The reward for taking the action is put into the pseudo &quot;post-state&quot;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">R&#x27;(post(s,a)) = gamma^(-1/2)R(s,a)</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">gamma&#x27; = gamma^(1/2)</span></div></div></div></div></div><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="value-iteration-algorithm"></a>Value Iteration Algorithm<a aria-hidden="true" tabindex="-1" class="hash-link" href="#value-iteration-algorithm" title="Direct link to heading">#</a></h1><ul><li>Repeatedly update the Utility function U(s) with bellman update.</li></ul><p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>U</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mi>R</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>+</mo><mi>Œ≥</mi><msub><mo><mi>max</mi><mo>‚Å°</mo></mo><mrow><mi>a</mi><mo>‚àà</mo><mi>A</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></msub><msub><mo>‚àë</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup></msub><mi>P</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup><mi mathvariant="normal">‚à£</mi><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><msub><mi>U</mi><mi>t</mi></msub><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">U_{t+1}(s) = R(s) + \gamma \max_{a\in A(s)} \sum_{s&#x27;} P(s&#x27;|s,a)U_t(s&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.107092em;vertical-align:-0.3551999999999999em"></span><span class="mord mathnormal" style="margin-right:0.05556em">Œ≥</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em"><span style="top:-2.5198em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mrel mtight">‚àà</span><span class="mord mathnormal mtight">A</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em">‚àë</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.17826999999999993em"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828285714285715em"><span style="top:-2.786em;margin-right:0.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">‚Ä≤</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚Ä≤</span></span></span></span></span></span></span></span></span><span class="mord">‚à£</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">‚Ä≤</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><ul><li>U is only updated after every single state has been looped through.</li></ul><div class="mdxCodeBlock_1XEh"><div class="codeBlockContent_1u-d"><button tabindex="0" type="button" aria-label="Copy code to clipboard" class="copyButton_10dd">Copy</button><div class="prism-code language-undefined codeBlock_3iAC"><div class="codeBlockLines_b7E3" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">// Returns Utility function U(s)</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">function Value-Iteration(mdp, err_threshold)</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    loop:</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        U_t = U_t+1, max_delta = 0</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        for all states s in S:</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">            Update U_t+1 with bellman equation.</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">            Update max_delta if abs(U(s) - U_t+1(s)) exceed max_delta</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        break loop if max_delta less than err_threshold(1-gamma)/gamma </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    return U_t</span></div></div></div></div></div><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="policy-evaluation-algorithm"></a>Policy Evaluation Algorithm<a aria-hidden="true" tabindex="-1" class="hash-link" href="#policy-evaluation-algorithm" title="Direct link to heading">#</a></h1><ul><li>Same as Value Iteration.
<strong> But we want to </strong>evaluate a given policy pi_i<strong>. Thus, <em>we don&#x27;t need to take the max, we just need to use the policy.</em></strong> Hence: Instead of taking the best action, take the action that the policy thinks is the best action</li></ul><p>[latexmath]
++++
U<em>i(s) = R(s) + \gamma \sum</em>{s&#x27;} P(s&#x27;|s,\pi_i(s))U_i(s&#x27;)
++++</p><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="policy-iteration-algorithm"></a>Policy Iteration Algorithm<a aria-hidden="true" tabindex="-1" class="hash-link" href="#policy-iteration-algorithm" title="Direct link to heading">#</a></h1><ul><li>We don&#x27;t need to calculate U(s) so accurately if we just want to find the optimal policy</li><li>Two step approach:</li></ul><ol><li><strong>Init</strong>: Start with initial policy pi_0</li><li><strong>Policy Evaluation</strong>: Calculate Utility function U_i given current policy. See Policy Evaluation above.</li><li><strong>Policy Improvement</strong>: Calculate new policy pi_i+1 using U_i.</li><li><strong>Termination</strong>: Repeat until the new and old policy are the same (no change).</li></ol><div class="mdxCodeBlock_1XEh"><div class="codeBlockContent_1u-d"><button tabindex="0" type="button" aria-label="Copy code to clipboard" class="copyButton_10dd">Copy</button><div class="prism-code language-undefined codeBlock_3iAC"><div class="codeBlockLines_b7E3" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">// Returns policy pi</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">function Policy-Iteration(mdp)</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    U = set all to 0</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    pi = random policy</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    loop</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        unchanged = true</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        U_i = Policy-Evaluation(pi, U, mdp)</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        for all states s in S:</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">            If the best action at s is different:</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">                Update pi[s]</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">                unchanged = false</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        break if unchanged</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    return pi</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    </span></div></div></div></div></div><p>The check for the best action is done as follows:</p><p>[latexmath]
++++
\max<em>{a\in A(s)} \sum</em>{s&#x27;} P(s&#x27;|s,a)U<em>i(s&#x27;) &gt; \sum</em>{s&#x27;} P(s&#x27;|s,\pi_i(s))U_i(s&#x27;)
++++</p><ul><li>Identify the action with highest utility at s</li><li>Check if it is the same action taken in the policy</li></ul><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="modified-policy-iteration"></a>Modified Policy Iteration<a aria-hidden="true" tabindex="-1" class="hash-link" href="#modified-policy-iteration" title="Direct link to heading">#</a></h1><ul><li>Only do k iterations instead of until no change</li></ul><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="model-based-prediction"></a>Model-Based Prediction<a aria-hidden="true" tabindex="-1" class="hash-link" href="#model-based-prediction" title="Direct link to heading">#</a></h1><ul><li>Prediction: Given traces of a policy and the final reward, learn the utility function by constructing the model from the data using an agent:
<strong> Learn </strong>transition model<strong> and </strong>reward function<strong> using an </strong>Adaptive Dynamic Programming<strong> (ADP) agent.
</strong> <strong>Calculate the Utility function.</strong></li></ul><div class="mdxCodeBlock_1XEh"><div class="codeBlockContent_1u-d"><button tabindex="0" type="button" aria-label="Copy code to clipboard" class="copyButton_10dd">Copy</button><div class="prism-code language-undefined codeBlock_3iAC"><div class="codeBlockLines_b7E3" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">// persistent variables</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">s_prev</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">pi: policy</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">mdp: current constructed model, rewards and discount</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">U: table of utilities</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">Nsa: Table keeping count of number of times at state s, action a was taken</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">Nsas&#x27;: Table keeping count of number of times the next state was s&#x27; given (s,a) (thus s,a,s&#x27;)</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">// Called everytime a new percept is observed by the agent</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">// Percept: (previous state s, current state s&#x27;, current reward r&#x27;)</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">function PASSIVE-ADP-AGENT(percept)</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    if s_curr is new:</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        U[s_curr] = r_curr</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        R[s_curr] = r_curr</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    if s_prev not null:</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        // Increment counters</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        Nsa[s_prev,a]++</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        Ns&#x27;sa[s_curr,s_prev,a]++</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        // Update transition function </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        // for every known reachable state by (s,a)</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        for every state where Ns&#x27;sa[state,s,a] &gt; 0:</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">            // # of times state happened when action was taken</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">            T(s,a,s&#x27;) = Ns&#x27;as[s,a,s&#x27;] / Nsa[s,a]</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    U = Policy-Evaluation(pi, U, mdp)</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    if s_curr is terminal:</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        s_prev,a = null</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    else</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        s_prev,a = s_curr, pi[s_curr]</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    return a</span></div></div></div></div></div><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="model-based-control"></a>Model-Based Control<a aria-hidden="true" tabindex="-1" class="hash-link" href="#model-based-control" title="Direct link to heading">#</a></h1><ul><li>Learn the policy, not the utility</li><li>Just replace Policy-Evaluation with Policy-Iteration</li><li>The policy no longer stays fixed but changes as transitions and rewards learnt</li><li>Note that however the algorithm is greedy and may not return the optimal value</li><li>Hence must do e-greedy exploration (choose a greedy action with 1-e probability and random action with e probability)
<strong> Greedy in the Limit of Infinite Exploration (GLIE), e = 1/t
</strong> Start with a high epsilon, then slowly reduce the number of random actions you take (by reducing the value of e) with every policy iteration as you become more and more certain you have an optimal algorithm</li></ul><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="monte-carlo-learning"></a>Monte Carlo Learning<a aria-hidden="true" tabindex="-1" class="hash-link" href="#monte-carlo-learning" title="Direct link to heading">#</a></h1><ul><li>Monte Carlo Learning (Direct Utility Estimation)
<strong> Given a series of states as a &quot;trial&quot; e.g. this is a trial: (1)-.04-&gt;(2)-.04-&gt;(2)-.04-&gt;(3)-.04-&gt;(4)-.04...-&gt;(5)+1
</strong> Keep a running reward average for every state; after infinite trials, sample average will converge to expected value
<strong>* e.g. for the above trial, maybe state (1) has a sample total reward of 0.72
**</strong> Calculate by taking sum of rewards from that state to the end)
<strong>* If a state is visited multiple times in a trial e.g. state (2):
**</strong> first-visit: take the sum of rewards from only on the first visit per trial
**** every visit: take multiple sums for every time the state was visited in a trial.</li></ul><p>Assume state s encountered k times with k returns, and each summed reward is stored in G_i(s). </p><ul><li>Note that G_i(s), given infinitely many i iterations, will converge to the true value; hence G_i(s) - U_k-1(s) is the MC error</li></ul><p>[latexmath]
++++
U<em>k(s) = \frac{1}{k}\sum</em>{i=1}^{k}G_i(s)
++++</p><p>[latexmath]
++++
U<em>k(s) = \frac{1}{k}G_k(s) + \frac{1}{k}\sum</em>{i=1}^{k-1}G_i(s)
++++</p><p>[latexmath]
++++
U<em>k(s) = \frac{1}{k}G_k(s) + \frac{1}{k}*(k-1)U</em>{k-1}(s)
++++</p><p>[latexmath]
++++
U<em>k(s) = \frac{k}{k}U</em>{k-1}(s) - \frac{1}{k}U_{k-1}(s) + \frac{1}{k}G_k(s)
++++</p><p>[latexmath]
++++
U<em>k(s) = U</em>{k-1}(s) + \frac{1}{k}(G<em>k(s) - U</em>{k-1}(s))
++++</p><ul><li>The difference between the current U_k(s) and the previous U(s) is the prediction error (if you want to mnimize absolute loss, you can use median instead of average)</li><li>Note that U(s) here can also apply to U(s,a), and can also be renamed as Q(s) and Q(s,a)</li></ul><div class="mdxCodeBlock_1XEh"><div class="codeBlockContent_1u-d"><button tabindex="0" type="button" aria-label="Copy code to clipboard" class="copyButton_10dd">Copy</button><div class="prism-code language-undefined codeBlock_3iAC"><div class="codeBlockLines_b7E3" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain"># Monte Carlo Prediction (Learning U(s))</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">After every trial:</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    Take every state (or first state occurence) and calculate its value sum</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    Increment Ns</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    U_k(s) = U_k-1(s) + 1/k(sum - U_k-1(s))</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    </span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"># Monte Carlo Control (Learning pi)</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">After every trial:</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    Take every (s,a):</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        newR(s,a) = take every/1st (s,a) and calculate value sum</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        Increment Ns</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        Q(s,a) = Q_k-1(s,a) + 1/k(newR(s,a) - Q_k-1(s,a))</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    For every s:</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        pi(s) = Take action that maximizes Q(s,a)</span></div></div></div></div></div><p>Advantages</p><ul><li>Simple</li><li>Unbiased estimate</li></ul><p>Disadvantages</p><ul><li>Must wait until full trial is done in order to perform learning</li><li>High variance (reward is the sum of many rewards along the trial), so need many trials to get it right</li></ul><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="temporal-difference-learning"></a>Temporal Difference Learning<a aria-hidden="true" tabindex="-1" class="hash-link" href="#temporal-difference-learning" title="Direct link to heading">#</a></h1><ul><li>Very similar to Monte Carlo Learning</li><li>Replace the </li></ul><p>[latexmath]
++++
U^\pi(s) = U^\pi(s) + \alpha(R(s) + \gamma U^\pi(s&#x27;) - U\pi(s))
++++</p><ul><li><p>Transition from state s to s&#x27;.</p></li><li><p>Alpha is the learning rate.
** Converges if alpha decreases with the number of times the state has been visited (think GLIE)</p></li><li><p>stem:[R(s) + \gamma U^\pi(s&#x27;)]: <strong>Temporal Difference target</strong>
** (this is basically bellman update, policy evaluation)</p></li><li><p>stem:[R(s) + \gamma U^\pi(s&#x27;) - U^\pi(s)]: <strong>Temporal Difference error</strong>
<strong> (amount of utility change from previous state s to current state s&#x27;)
</strong> difference between the estimated reward at any given state or time step and the actual reward received
** Utility of current state + discounted future reward(future state) - expectedRewardWithIncludesFutureRewards(s)</p></li></ul><div class="mdxCodeBlock_1XEh"><div class="codeBlockContent_1u-d"><button tabindex="0" type="button" aria-label="Copy code to clipboard" class="copyButton_10dd">Copy</button><div class="prism-code language-undefined codeBlock_3iAC"><div class="codeBlockLines_b7E3" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain"># Prediction</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">TD-Agent:</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">for every percept (curr_state s&#x27;, immediate reward r&#x27;)</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">if s&#x27; is new:</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    U[s&#x27;] = r&#x27;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">if s not null:</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    Ns[s]++</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    U[s] = U[s] + alpha*Ns[s]*(r+gamma*U[s&#x27;]-U[s])</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">if s&#x27; terminal:</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    s,a,r = null</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">else: # update prev data</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    s,a,r = s&#x27;,pi[s&#x27;],r&#x27;</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">return a</span></div></div></div></div></div><p>Advantages</p><ul><li>Can learn with every step (in a trial)</li><li>Usually converges faster in practice</li></ul><p>Disadvantages</p><ul><li>Online; lower variance, but estimate on how good your estimate is; biased</li><li>Assumes MDP</li><li>Error can go in any direction</li></ul><p>SARSA uses TD learning w.r.t. a specific policy:</p><p>[latexmath]
++++
U^\pi(s,a) = U^\pi(s,a) + \alpha(R(s,a) + \gamma U^\pi(s&#x27;,\pi((s&#x27;)) - U\pi(s,a))
++++</p><p>Q-Learning uses TD learning w.r.t. the optimal policy (s&#x27; is the next state after (s,a) is taken):</p><p>[latexmath]
++++
Q(s,a) = Q(s,a) + \alpha(R(s,a) + \gamma \max_{a\in A(s&#x27;)} Q(s&#x27;,a) - Q(s,a))
++++</p><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="n-step-td-tdlambda"></a>n-step TD, TD(lambda)<a aria-hidden="true" tabindex="-1" class="hash-link" href="#n-step-td-tdlambda" title="Direct link to heading">#</a></h1><p>Alister Reis has a very good blog post on this
<a href="https://amreis.github.io/ml/reinf-learn/2017/11/02/reinforcement-learning-eligibility-traces.html" target="_blank" rel="noopener noreferrer">https://amreis.github.io/ml/reinf-learn/2017/11/02/reinforcement-learning-eligibility-traces.html</a></p><p>See also?</p><ul><li><a href="https://towardsdatascience.com/reinforcement-learning-td-%CE%BB-introduction-686a5e4f4e60#:~:text=random%20walk%20example-,The%20Idea%20of%20TD(%CE%BB),0.5*Gt%3At%2B4" target="_blank" rel="noopener noreferrer">https://towardsdatascience.com/reinforcement-learning-td-%CE%BB-introduction-686a5e4f4e60#:~:text=random%20walk%20example-,The%20Idea%20of%20TD(%CE%BB),0.5*Gt%3At%2B4</a></li></ul><p>Other Resources:</p><ul><li></li></ul></div></article><div class="margin-vert--xl"><div class="row"><div class="col"><a href="https://github.com/crazoter/My-Docs/edit/master/website/docs/markdown/ai_planning.md" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="1.2em" width="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 40 40" style="margin-right:0.3em;vertical-align:sub"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/My-Docs/docs/docusaurus/mdx"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">¬´ Powered by MDX</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/My-Docs/docs/markdown/machine_learning"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Machine Learning ¬ª</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><h4 class="footer__title">Links</h4><ul class="footer__items"><li class="footer__item"><a href="https://www.linkedin.com/in/matthew-lee-6a8a8a70/" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn</a></li><li class="footer__item"><a href="https://crazoter.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">Portfolio</a></li></ul></div></div><div class="text--center"><div>Copyright ¬© 2020 Matt's Docs, crazoter. Built with Docusaurus.</div></div></div></footer></div>
<script src="/My-Docs/styles.2c9d9d1b.js"></script>
<script src="/My-Docs/runtime~main.9dbf4ab6.js"></script>
<script src="/My-Docs/main.37720434.js"></script>
<script src="/My-Docs/common.33db9307.js"></script>
<script src="/My-Docs/2.6eb0d358.js"></script>
<script src="/My-Docs/28.f7ca7a2d.js"></script>
<script src="/My-Docs/29.d9316f6b.js"></script>
<script src="/My-Docs/935f2afb.3211b5cf.js"></script>
<script src="/My-Docs/17896441.fdf12432.js"></script>
<script src="/My-Docs/4910ebf8.77fdcaa0.js"></script>
</body>
</html>