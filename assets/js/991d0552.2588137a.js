"use strict";(self.webpackChunkmy_docs=self.webpackChunkmy_docs||[]).push([[2071],{3905:function(e,a,t){t.d(a,{Zo:function(){return o},kt:function(){return u}});var n=t(7294);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function l(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?i(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var m=n.createContext({}),p=function(e){var a=n.useContext(m),t=a;return e&&(t="function"==typeof e?e(a):l(l({},a),e)),t},o=function(e){var a=p(e.components);return n.createElement(m.Provider,{value:a},e.children)},N="mdxType",k={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},c=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,i=e.originalType,m=e.parentName,o=s(e,["components","mdxType","originalType","parentName"]),N=p(t),c=r,u=N["".concat(m,".").concat(c)]||N[c]||k[c]||i;return t?n.createElement(u,l(l({ref:a},o),{},{components:t})):n.createElement(u,l({ref:a},o))}));function u(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var i=t.length,l=new Array(i);l[0]=c;var s={};for(var m in a)hasOwnProperty.call(a,m)&&(s[m]=a[m]);s.originalType=e,s[N]="string"==typeof e?e:r,l[1]=s;for(var p=2;p<i;p++)l[p]=t[p];return n.createElement.apply(null,l)}return n.createElement.apply(null,t)}c.displayName="MDXCreateElement"},385:function(e,a,t){t.r(a),t.d(a,{assets:function(){return o},contentTitle:function(){return m},default:function(){return u},frontMatter:function(){return s},metadata:function(){return p},toc:function(){return N}});var n=t(7462),r=t(3366),i=(t(7294),t(3905)),l=["components"],s={title:"Python Reference"},m=void 0,p={unversionedId:"markdown/python",id:"markdown/python",title:"Python Reference",description:"These are my notes from datacamp.",source:"@site/docs/markdown/python.md",sourceDirName:"markdown",slug:"/markdown/python",permalink:"/My-Docs/docs/markdown/python",draft:!1,editUrl:"https://github.com/crazoter/My-Docs/edit/main/docs/markdown/python.md",tags:[],version:"current",frontMatter:{title:"Python Reference"},sidebar:"someSidebar",previous:{title:"CS4268 Quantum Computing",permalink:"/My-Docs/docs/markdown/CS4268"},next:{title:"Math",permalink:"/My-Docs/docs/markdown/math"}},o={},N=[{value:"Project setup",id:"project-setup",level:2},{value:"Pure Python",id:"pure-python",level:2},{value:"Libraries / Modules",id:"libraries--modules",level:2},{value:"Regex",id:"regex",level:3},{value:"Recordlinkage (Join datasets w/o common UID)",id:"recordlinkage-join-datasets-wo-common-uid",level:3},{value:"fuzzywuzzy (String Comparison)",id:"fuzzywuzzy-string-comparison",level:3},{value:"missingno (Visualize missing data)",id:"missingno-visualize-missing-data",level:3},{value:"scipy.stats (zscore)",id:"scipystats-zscore",level:3},{value:"Textatistic (Evaluate word readability)",id:"textatistic-evaluate-word-readability",level:3},{value:"spacy (tokenization and lemmatization)",id:"spacy-tokenization-and-lemmatization",level:3},{value:"matplotlib.pyplot (Graphs &amp; Images)",id:"matplotlibpyplot-graphs--images",level:3},{value:"Seaborn (Graphs)",id:"seaborn-graphs",level:3},{value:"scikit-learn",id:"scikit-learn",level:3},{value:"Verifying model accuracy",id:"verifying-model-accuracy",level:2},{value:"Hyperparam Tuning",id:"hyperparam-tuning",level:2},{value:"Imputer (Impute missing info)",id:"imputer-impute-missing-info",level:3},{value:"Numpy",id:"numpy",level:2}],k={toc:N},c="wrapper";function u(e){var a=e.components,s=(0,r.Z)(e,l);return(0,i.kt)(c,(0,n.Z)({},k,s,{components:a,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"These are my notes from datacamp."),(0,i.kt)("p",null,"Legend:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"#: Number, used for differentiating variables"),(0,i.kt)("li",{parentName:"ul"},"L#: List."),(0,i.kt)("li",{parentName:"ul"},"D#: Dictionary. "),(0,i.kt)("li",{parentName:"ul"},"itr#: Iterable."),(0,i.kt)("li",{parentName:"ul"},"DF#: Dataframe"),(0,i.kt)("li",{parentName:"ul"},"mltIdx#: MultiIndex"),(0,i.kt)("li",{parentName:"ul"},"idx: Index"),(0,i.kt)("li",{parentName:"ul"},"int#: Integer variable")),(0,i.kt)("h2",{id:"project-setup"},(0,i.kt)("a",{parentName:"h2",href:"https://docs.python-guide.org/writing/structure/"},"Project setup")),(0,i.kt)("h2",{id:"pure-python"},"Pure Python"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Exponentiation: val ** power"),(0,i.kt)("li",{parentName:"ul"},"Type conversion: ",(0,i.kt)("em",{parentName:"li"},"type"),"(val), where ",(0,i.kt)("em",{parentName:"li"},"type")," in {int, float, str, bool}"),(0,i.kt)("li",{parentName:"ul"},"Iterables",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Loop with Index"),": for idx, val in enumerate(itr)"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Generators (lazy loading iteration)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Creation: ",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"gen = iter(L / D / *range(i)*)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"gen = (x for x in list)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"gen = def fx(param): ... for x in param: yield x ...")))),(0,i.kt)("li",{parentName:"ul"},"Use:    val = next(itr)"))))),(0,i.kt)("li",{parentName:"ul"},"Lists",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Init with List Comprehension"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Format: ",(0,i.kt)("inlineCode",{parentName:"li"},"[ (value) for (var_name) in (iterable) if (predicate) ]"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"e.g. [L","[0]"," for elem in list]"))))),(0,i.kt)("li",{parentName:"ul"},"List of tuples:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Convert L to L of indexed tuples:"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"enumerate(itr, start=int1) = [(int1,itr[0]), (int1+1,itr[0])...]")))),(0,i.kt)("li",{parentName:"ul"},"Merge two lists into a list of tuples:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"zip(*L1*,*L2*)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"returns zipObject: [ (L1","[0]",",L2","[0]","),  (L1","[1]",",L2","[1]",")... ]"),(0,i.kt)("li",{parentName:"ul"},"Access zipObject contents: (*zipObj)"))),(0,i.kt)("li",{parentName:"ul"},"Unzip: ",(0,i.kt)("inlineCode",{parentName:"li"},"zip(*zipObj)")))))),(0,i.kt)("li",{parentName:"ul"},"Count occurrences: ",(0,i.kt)("inlineCode",{parentName:"li"},"list.count(obj)")))),(0,i.kt)("li",{parentName:"ul"},"Dictionaries",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Init with Dictionary Comprehension"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Format: ",(0,i.kt)("inlineCode",{parentName:"li"},"{ (key : value) for (var_name) in (iterable) if (predicate) }"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"e.g. {x : len(x) for x in list}"))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Init from list of tuples"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"dict(zip(L1,L2))")))))),(0,i.kt)("li",{parentName:"ul"},"Functions:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Default params: x=default"),(0,i.kt)("li",{parentName:"ul"},"Flexible list param: ",(0,i.kt)("inlineCode",{parentName:"li"},"f(*args)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Usage: f(v1,v2,v3...)"))),(0,i.kt)("li",{parentName:"ul"},"Flexible dict param: ",(0,i.kt)("inlineCode",{parentName:"li"},"f(**kwargs)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Usage: f(k1=v1,k2=v2,k3=v3...)"))),(0,i.kt)("li",{parentName:"ul"},"Multiple output: ",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"def fx(): return (x, y)"),(0,i.kt)("li",{parentName:"ul"},"Multiple assignment:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"x,y = fx()"))))),(0,i.kt)("li",{parentName:"ul"},"Global variables: ",(0,i.kt)("inlineCode",{parentName:"li"},"global varname")),(0,i.kt)("li",{parentName:"ul"},"Nested functions:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Variables & params of external f() is accessible"),(0,i.kt)("li",{parentName:"ul"},"The function itself can be returned"),(0,i.kt)("li",{parentName:"ul"},"Modify variables from nested f(): ",(0,i.kt)("inlineCode",{parentName:"li"},"nonlocal varname")))))),(0,i.kt)("li",{parentName:"ul"},"Functional Programming",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"map((lambda a: (transformation)), L)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"filter((lambda a: (predicate)), L)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"reduce((lambda a,b: ...), L) = result"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Import: ",(0,i.kt)("inlineCode",{parentName:"li"},"from functools import reduce")))),(0,i.kt)("li",{parentName:"ul"},"The lambda can be replaced with a concrete function"))),(0,i.kt)("li",{parentName:"ul"},"Lambdas",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"(lambda (params): (body))")),(0,i.kt)("li",{parentName:"ul"},"e.g. (lambda a: a+1) \u2261 def f(a): return a+1"),(0,i.kt)("li",{parentName:"ul"},'No "return"'),(0,i.kt)("li",{parentName:"ul"},"No multi-line"))),(0,i.kt)("li",{parentName:"ul"},"Exception Handling",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"try: ... "),(0,i.kt)("li",{parentName:"ul"},"except: ... "),(0,i.kt)("li",{parentName:"ul"},"raise ",(0,i.kt)("em",{parentName:"li"},"Error"),"(",(0,i.kt)("em",{parentName:"li"},"msg"),")",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Error = {ValueError, TypeError, YourOwnErrClass}"))))),(0,i.kt)("li",{parentName:"ul"},"I/O",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Open file:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"with open('filepath') as file_var"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"file_var.readline()"),": returns None if empty"))))),(0,i.kt)("li",{parentName:"ul"},"Get script directory:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"dir_path = os.path.dirname(os.path.realpath(__file__))")))),(0,i.kt)("li",{parentName:"ul"},"Get path to file relative to script directory:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"local_file = os.path.join(dir_path, 'path', 'to', 'local_file')")))))),(0,i.kt)("li",{parentName:"ul"},"Datetime",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Req: ",(0,i.kt)("inlineCode",{parentName:"li"},"import datetime as dt")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"dt.date.today()"))))),(0,i.kt)("h2",{id:"libraries--modules"},"Libraries / Modules"),(0,i.kt)("h3",{id:"regex"},"Regex"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Create Pattern"),": ",(0,i.kt)("inlineCode",{parentName:"li"},'pattern = re.compile(r"regex_pattern")')),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Match"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"matches = re.match(pattern, str)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Returns None if no matches found"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Get found values"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"matches.group(n), n = 0 if no groups defined")," "))),(0,i.kt)("li",{parentName:"ul"})),(0,i.kt)("h3",{id:"recordlinkage-join-datasets-wo-common-uid"},"Recordlinkage (Join datasets w/o common UID)"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"import recordlinkage")),(0,i.kt)("li",{parentName:"ul"},"Purpose: ",(0,i.kt)("strong",{parentName:"li"},"Join different datasets when they don't share a unique identifier.")," See ",(0,i.kt)("a",{parentName:"li",href:"https://recordlinkage.readthedocs.io/en/latest/ref-index.html"},"Documentation"),(0,i.kt)("ol",{parentName:"li"},(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Init an indexer"))),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"idxr = recordlinkage.Index()"))),(0,i.kt)("ol",{parentName:"li",start:2},(0,i.kt)("li",{parentName:"ol"},"Blocking: only ",(0,i.kt)("strong",{parentName:"li"},"choose pairs of entries that have the same value under specified column"),' (e.g. "cuisine_type")')),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},'idxr.block("col_name")'))),(0,i.kt)("ol",{parentName:"li",start:3},(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Generate said pairs of indexes")," which agree on the equal columns")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"mltIdx_pairs = idxr.index(df1, df2)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Example pair: MultiIndex(","[(0,0),(0,1),(0,7),(1,0),(1,4)...]",")")))),(0,i.kt)("ol",{parentName:"li",start:4},(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Specify the columns to compare")," with a Compare object")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"comp = recordlinkage.Compare()"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Then, specify the columns to compare by:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"comp.exact('col_nm_in_df1', 'col_nm_in_df2', label='new_lbl_in_new_df')"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Entries must ",(0,i.kt)("strong",{parentName:"li"},"exact match")," in the columns"),(0,i.kt)("li",{parentName:"ul"},"e.g. comp.exact('city', 'city', label='city')"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"comp.string('col_nm_in_df1', 'col_nm_in_df2', label='new_lbl_in_new_df', threshold = dbl_frm_0-1)")," (threshold usually 0.8)"),(0,i.kt)("li",{parentName:"ul"},"Entries must be ",(0,i.kt)("strong",{parentName:"li"},"similar")," (in terms of string) in the columns")))))),(0,i.kt)("ol",{parentName:"li",start:5},(0,i.kt)("li",{parentName:"ol"},"Apply the Compare object to ",(0,i.kt)("strong",{parentName:"li"},"get a dataframe highlighting potential matches"))),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"df_pttl_mtchs = comp.compute(mltIdx_pairs, df1, df2)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Structure of df_pttl_mtchs:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Index / Col 0: mltIdx_pairs"),(0,i.kt)("li",{parentName:"ul"},"Columns: columns used for comparison",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"exact: 1 if equal else 0"),(0,i.kt)("li",{parentName:"ul"},"string: 1 if threshold met, else 0")))))))),(0,i.kt)("ol",{parentName:"li",start:6},(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Filter matches")," from potential matches")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"df_mtchs = df_pttl_mtchs[df_pttl_mtchs.sum(axis=1) >= 3]"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"If the val == 1 then there's a match on that column. This counts the number of matched columns, and filters by that."),(0,i.kt)("li",{parentName:"ul"},"In this case there were 3 columns so 3 was chosen")))),(0,i.kt)("ol",{parentName:"li",start:7},(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Select matched indexes")," for one of the DFs (in this case df2)",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"idx_df2_mtched = df_mtchs.index.get_level_values(1)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"df_pttl_mtchs & df_mtchs use a MultiIndex. "),(0,i.kt)("li",{parentName:"ul"},"df_mtchs.index.get_level_values(0) = df1's indexes, (1) = df2's indexes"))))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"From df2, remove entries that match df1's entries"))),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"df2_notInDf1 = df2[~df2.index.isin(idx_df2_mtched)]"))),(0,i.kt)("ol",{parentName:"li",start:9},(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Join df1 and the new df2 entries"))),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"df1.append(df2_notInDf1)"))))),(0,i.kt)("h3",{id:"fuzzywuzzy-string-comparison"},"fuzzywuzzy (String Comparison)"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Import"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"from fuzzywuzzy import process")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Test similarity:")," ",(0,i.kt)("inlineCode",{parentName:"li"},"process.extract('target_word', arr_of_candidate_words, length_of_arr)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Returns array of tuples: ",(0,i.kt)("inlineCode",{parentName:"li"},"[('candidate_word', similarity_score), ...]"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"similarity_score: from 0 - 100, 100 as perfect, 80 as close enough")))))),(0,i.kt)("h3",{id:"missingno-visualize-missing-data"},"missingno (Visualize missing data)"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"import missingno as msno")),(0,i.kt)("li",{parentName:"ul"},"msno.matrix(df_with_missingvals); plt.show()")),(0,i.kt)("h3",{id:"scipystats-zscore"},"scipy.stats (zscore)"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"from scipy.stats import zscore")),(0,i.kt)("li",{parentName:"ul"},"calculate zscore values: ",(0,i.kt)("inlineCode",{parentName:"li"},"zscore(df['col'])"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"z-score is the number of standard deviations by which an observation is above the mean - so if it is negative, it means the observation is below the mean.")))),(0,i.kt)("h3",{id:"textatistic-evaluate-word-readability"},"Textatistic (Evaluate word readability)"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"from textatistic import Textatistic")),(0,i.kt)("li",{parentName:"ul"},"Compute scores: ",(0,i.kt)("inlineCode",{parentName:"li"},"scores = Textatistic(article/string).scores")),(0,i.kt)("li",{parentName:"ul"},"Get Flesch score: ",(0,i.kt)("inlineCode",{parentName:"li"},"scores['flesch_score']")),(0,i.kt)("li",{parentName:"ul"},"Get gunningfog: ",(0,i.kt)("inlineCode",{parentName:"li"},"scores['gunningfog_score']"))),(0,i.kt)("h3",{id:"spacy-tokenization-and-lemmatization"},"spacy (tokenization and lemmatization)"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"import spacy")),(0,i.kt)("li",{parentName:"ul"},"Tokenization")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"import spacy\n\n# Load the en_core_web_sm model which comes with the spaCy library (see https://spacy.io/models/en)\nnlp = spacy.load('en_core_web_sm')\n\n# Create a Doc object\ndoc = nlp(gettysburg)\n\n# Generate the tokens\ntokens = [token.text for token in doc]\n")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Lemmatization (accuracy dependent on moduel)")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"import spacy\n\n# Load the en_core_web_sm model\nnlp = spacy.load('en_core_web_sm')\n\n# Create a Doc object\ndoc = nlp(gettysburg)\n\n# Generate lemmas (accuracy dependent on model)\nlemmas = [token.lemma_ for token in doc]\n\n# Convert lemmas into a string\nprint(' '.join(lemmas))\n")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Preprocess with lemmatization, removing non alphabeticals")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"# Function to preprocess text\ndef preprocess(text):\n    # Create Doc object\n    doc = nlp(text, disable=['ner', 'parser'])\n    # Generate lemmas\n    lemmas = [token.lemma_ for token in doc]\n    # Remove stopwords and non-alphabetic characters\n    a_lemmas = [lemma for lemma in lemmas \n            if lemma.isalpha() and lemma not in stopwords]\n    \n    return ' '.join(a_lemmas)\n  \n# Apply preprocess to ted['transcript']\nted['transcript'] = ted['transcript'].apply(preprocess)\nprint(ted['transcript'])\n")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"POS (piece-of-speech) tagging ")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"# Load the en_core_web_sm model\nnlp = spacy.load('en_core_web_sm')\n\n# Create a Doc object\ndoc = nlp(lotf)\n\n# Generate tokens and pos tags\npos = [(token.text, token.pos_) for token in doc]\nprint(pos) \n# Output: [('He', 'PRON'), ('found', 'VERB'), ('himself', 'PRON'), ('understanding', 'VERB') ...\n")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Named Entities Recognition (NER)")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"# Load the required model\nnlp = spacy.load('en_core_web_sm')\n\n# Create a Doc instance \ntext = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\ndoc = nlp(text)\n\n# Print all named entities and their labels\nfor ent in doc.ents:\n    print(ent.text, ent.label_)\n\"\"\"\nOutput:\n    Sundar Pichai ORG\n    Google ORG\n    Mountain View GPE\n\"\"\"\n\n# Alternatively:\ndef find_persons(text):\n  # Create Doc object\n  doc = nlp(text)\n  \n  # Identify the persons\n  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n  \n  # Return persons\n  return persons\n\nprint(find_persons(tc))\n")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Word embeddings"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Compare similarities")),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"# Create the doc object\ndoc = nlp(sent)\n\n# Compute pairwise similarity scores\nfor token1 in doc:\n  for token2 in doc:\n    print(token1.text, token2.text, token1.similarity(token2))\n# Similarity between 2 documents\nnlp(doc1).similarity( nlp(doc2) )\n")))),(0,i.kt)("h3",{id:"matplotlibpyplot-graphs--images"},"matplotlib.pyplot (Graphs & Images)"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"import matplotlib.pyplot as plt")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Think of ",(0,i.kt)("inlineCode",{parentName:"p"},"plt")," as some kind of a global variable to attach stuff to")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Change Styles:"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Use style"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.style.use(style_name)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Show available styles"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.style.available")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Improve the spacing between subplots"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.tight_layout()")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Hide gridlines"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.grid('off')")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Hide axes"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.axis('off')")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Graphs"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Create multiple graphs:"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Automatically using ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.subplot(rows, columns, active_subplot_idx)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Call the function with a new ",(0,i.kt)("inlineCode",{parentName:"li"},"active_subplot_idx")," = row x rowlen * column to change the current graph.",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"active_subplot_idx")," starts from 1"))))),(0,i.kt)("li",{parentName:"ul"},"Specify axes directly: "),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Define bounding box (axes)"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.axes([xlower, ylower, width_%, height_%])"),", args passed as a list",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Think of these as rectangle bounds of your current graph.",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"e.g. if you want 2 graphs side-by-side: ",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"First set the axes for the left one ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.axes([0.05, 0.05, 0.425, 0.9])")," "),(0,i.kt)("li",{parentName:"ul"},"Then plot the left graph ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.plot(year, physical_sciences, color='blue')")),(0,i.kt)("li",{parentName:"ul"},"The set the axes for the right one ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.axes([0.525, 0.05, 0.425, 0.9])")),(0,i.kt)("li",{parentName:"ul"},"and plot that graph ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.plot(year, computer_science, color='red')")))))))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Plot graph in active subplot:"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Plot line"),":")),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"plt.plot(\n    OPTION 1 WITH TWO LISTS: x_positions_of_points, y_positions_of_points, \n    OPTION 2 WITH 1 DATAFRAME: dataframe_of_x_and_y \n    color='blue', \n    label=str     # Used to label the line in the legend\n)\n")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Scatter"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.scatter(x_data, y_data, label='data', color='red', marker='o')")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Histogram"),": ")),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"plt.hist(pixels, bins=64, \n    range=(0, 256),   # x-axis range\n    normed=True,      # normalized histogram\n    cumulative=True,  # cumulative density function instead of probability density function\n    color='red', \n    alpha=0.4,\n    histtype='bar'      # https://matplotlib.org/3.2.1/gallery/statistics/histogram_histtypes.html\n)\n\n# Alternatively\ndataframe.hist()\n")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Empirical CDF"),":  ")),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"def ecdf(data):\n    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n    # Return x and y data\n    return (np.sort(data), np.arange(1, len(data)+1) / n)\n\nx1, y1 = ecdf(data_list1)\nx2, y2 = ecdf(data_list2)\nplt.plot(x1, y1, marker='.', linestyle = 'none')\nplt.plot(x2, y2, marker='.', linestyle = 'none') # marker='D' for diamonds\nplt.legend(('data 1', 'data 2'), loc='lower right')\nplt.show()\n")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Boxplot"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"dataframe.boxplot(column = [y_axis_col_values], by=[x_axis_col_values])")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Add title"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.title(str)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Change x and y labels"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.xlabel(str)")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.ylabel(str)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Change x and y limits (set range)"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.xlim(lower,upper)")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.ylim(lower,upper)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Inclusive"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Change both at the same time"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.axis((x_lower,x_upper,y_lower, y_upper))")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Configuring xticks / yticks"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.xticks(rotation=degs)")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.yticks(rotation=degs)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"ticks: the markers showing the coordinates on the x and y axis"),(0,i.kt)("li",{parentName:"ul"},"rotation: angle at which ticks are displayed"))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Add legend"),": ",(0,i.kt)("inlineCode",{parentName:"p"},"plt.legend((label1, label2, ...), loc='lower center')"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Overlay plots"),": ",(0,i.kt)("inlineCode",{parentName:"p"},"plt.twinx()")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Use plt.twinx() to overlay plots with different vertical scales on a common horizontal axis."))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Annotate")),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"plt.annotate(text, \n    xy=(x_pos, y_pos),    # xy of value you're pointing to\n    xytext=(x_pos, y_pos), \n    arrowprops=dict(facecolor='black')\n)\n"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"2D histogram"),":"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"plt.hist2d(horizontal_data, vertical_data, bins=(x_cols, y_rows), range=((x_min, x_max), (y_min, y_max)))")),(0,i.kt)("li",{parentName:"ul"},"Range is optional"),(0,i.kt)("li",{parentName:"ul"},"Instead of plotting the points directly on a graph, you turn it into something of a density map; the graph is split into a grid, and boxes with a lot of points will have a color of higher intensity."))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"2D hex histogram"),":"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"plt.hexbin(horizontal_data, vertical_data, gridsize=(x_cols, y_rows), extent=(x_min, x_max, y_min, y_max))")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Display points w/ color"),": ",(0,i.kt)("inlineCode",{parentName:"p"},"plt.pcolor(2D_arr,...)")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Add param ",(0,i.kt)("inlineCode",{parentName:"li"},"cmap='Blues'")," to config colormapping to Blues"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Display color and intensity mapping"),": ",(0,i.kt)("inlineCode",{parentName:"p"},"plt.colorbar()")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Note that the bottom left part of the image maps to the top left part of the numpy array"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Display points as contours"),": ",(0,i.kt)("inlineCode",{parentName:"p"},"plt.contour(X, Y, Z, contour_count, cmap='color_map')")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Filled: ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.contourf(...)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://jakevdp.github.io/PythonDataScienceHandbook/04.04-density-and-contour-plots.html"},"See")))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Images"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Load image"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"npRGB = plt.imread(filepath)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Display image"),": ")),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"plt.imshow(npRGB,\n    cmap='gray',\n    extent=(-1, 1, -1, 1) # horizontal extent from -1 to 1, vertical extent from -1 to 1\n    aspect=1              # aspect ratio (# of vertical pixels : # of horizontal pixels). \n                          # \\lt 1 means img is squashed downwards; \\gt 1 means img is stretched upwards\n)\n")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Add cmap if only 1 channel"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"RGB to monochannel"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"npRGB.sum(axis=2)")),(0,i.kt)("li",{parentName:"ul"},"Others:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Split RGB into channels:"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"red, green, blue = img[:,:,0], img[:,:,1], img[:,:,2]")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Flatten monochannel image (without modifying values) into 1-D array"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"image.flatten()")))),(0,i.kt)("li",{parentName:"ul"},"Normalize intensity:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"256*(img-img.min())/(img.max()-img.min())")))))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Export:"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Show on GUI"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.show()")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Prepare graph on another figure"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.figure()")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Save to file"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.savefig(filepath)"))))),(0,i.kt)("h3",{id:"seaborn-graphs"},"Seaborn (Graphs)"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("inlineCode",{parentName:"p"},"import seaborn as sns"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Set default style: ",(0,i.kt)("inlineCode",{parentName:"p"},"sns.set()"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Simple linear regression"),":"),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"sns.lmplot(x='x_pos_col_in_df', y='y_pos_col_in_df', data=dataframe\n    hue='categorical_col_in_df', # This col is categorical; will be used to group the points by colour\n    row='groupby_row_wise'\n    palette='Set1'\n)\n")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"straight line best fit"),(0,i.kt)("li",{parentName:"ul"},"hue: e.g. you have a col \"gender\" that allows only {'M','F'}. ",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Graph will color points that have value 'M' to one color, and 'F' to another color (i.e. groupby)"))),(0,i.kt)("li",{parentName:"ul"},"row: same purpose as hue, but groupby row-wise",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"i.e. Segregate points by each category and plot a separate graph for each"))),(0,i.kt)("li",{parentName:"ul"},"plots on current plt graph. Use ",(0,i.kt)("inlineCode",{parentName:"li"},"plt.show()")," to show "))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"2nd order regression"),": "),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"sns.regplot(x='x_col_in_df', y='y_col_in_df', data=dataframe,\n    color='green', \n    scatter=None,         # Set scatter to None if you don't want to plot the scatter points; else ignore this line\n    order=2,              # 1 for simple lin. regr., 2 for 2nd order etc\n    label='legend_label'  # label for legend\n)\n")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"curved line best fit"),(0,i.kt)("li",{parentName:"ul"}))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Residual plot"),": ",(0,i.kt)("inlineCode",{parentName:"p"},"sns.residplot(x='x_col_in_df', y='y_col_in_df', data=dataframe, color=color_str)")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},'Visualize how far datapoints diverge from the regression line (a "residual" is the distance from a datapoint to the line)'))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"box plot"),": ",(0,i.kt)("inlineCode",{parentName:"p"},"sns.boxplot(x='east_west', y='dem_share', data=df_all_states)"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Strip plot"),": A scatter plot where the x axis represents a categorical variable."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"sns.stripplot(x='x_col_in_df', y='y_col_in_df', data=df,\n    size = n,   # Size of dots\n    jitter=True # Useful when many points overlap, easier to see distribution. \n)\n")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://seaborn.pydata.org/generated/seaborn.stripplot.html"},"Documentation")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Swarm plot"),": Similar to strip plot, but the points visually spread out to avoid overlap"),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"sns.swarmplot(x='x_col_in_df', y='y_col_in_df', data=df,\n    orient = 'h'/'v'  # h: y is now the categorical var. v: same as stripplot\n    size = n,         # Size of dots\n    hue='col',        # Categorical column to colour points by\n)\n"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Violin plot"),": Similar to a box plot, with the addition of a rotated kernel density plot on each side"),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"sns.violinplot(x='x_col_in_df', y='y_col_in_df', data=df,\n    color='lightgray',  # If you want all violins to be of the same color\n    inner=None          # Points are visualized in the center of each x coord. inner=None to only show the violin body.\n)\n"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Joint plot"),": Main plot in the middle defined by ",(0,i.kt)("inlineCode",{parentName:"p"},"kind"),", combined with histograms aligned to the x and y axis at the side."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"sns.jointplot(x='hp', y='mpg',data=auto,\n    kind = 'scatter' | 'reg' | 'resid' | 'kde' | 'hex'\n)\n")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"kind='scatter': scatter plot of the data points"),(0,i.kt)("li",{parentName:"ul"},"kind='reg': regression plot (default order 1)"),(0,i.kt)("li",{parentName:"ul"},"kind='resid': residual plot"),(0,i.kt)("li",{parentName:"ul"},"kind='kde': kernel density estimate of the joint distribution"),(0,i.kt)("li",{parentName:"ul"},"kind='hex': hexbin plot of the joint distribution"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Pair(wise) plot"),": Take every pairwise combination of every non-categorical column in dataframe and plot main plot + histogram."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"sns.pairplot(df, kind = 'scatter' | 'reg' | 'resid' | 'kde' | 'hex'\n    hue = 'categorical_col' # Categorical column to colour points by\n)```\n\n"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Heatmap"),": Good for visualizing 2D arrays (e.g. covariance matrices)"),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"sns.heatmap(df, linewidths=1, linecolor='black')\n"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Countplot"),": Good for binary features"),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"sns.countplot(x='education', hue='party', data=df, palette='RdBu')\n")))),(0,i.kt)("h3",{id:"scikit-learn"},"scikit-learn"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"import sklearn"),"\n",(0,i.kt)("strong",{parentName:"p"},"train_test_split")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.model_selection import train_test_split")),(0,i.kt)("li",{parentName:"ul"},"Use stratified sampling to split up the dataset according to the categorical_y_data dataset",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"X_train, X_test, y_train, y_test = train_test_split(data_x, categorical_y_data, stratify=categorical_y_data, test_size=<0.0 - 1.0>, random_state=seed)")),(0,i.kt)("li",{parentName:"ul"},"Stratified: Make the distribution of each feature as close as possible to the original in the training and test sets"),(0,i.kt)("li",{parentName:"ul"},"75% into training set and 25% into test set")))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"mean_squared_error")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.metrics import mean_squared_error")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"rmse = np.sqrt(mean_squared_error(y_test, y_pred))"))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Pipelines")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.pipeline import Pipeline")),(0,i.kt)("li",{parentName:"ul"},"Usage: ")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"pipe = Pipeline([\n  ('scaler',StandardScaler()),              # 0\n  ('reducer', PCA()),                       # 1\n  ('classifier', RandomForestClassifier())  # 2 ('SVM', SVC()) | ('elasticnet', ElasticNet())\n])\n# pipe.fit(Xtrain, ytrain)\n# pipe.fit_transform(...)\n# accessing the tuple ('scaler', scaler_obj): pipe[0]\n")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Concepts")),(0,i.kt)("p",null,"Data Standardization (normalization):"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Preprocessing task performed on numerical, continuous data to ",(0,i.kt)("strong",{parentName:"li"},"make it normally distributed"),". Standardize (assuming linear space) when:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li"},(0,i.kt)("li",{parentName:"ol"},"Using a model that is in a linear space (any kind of model that uses a linear distance metric or operates in a linear space like k-nearest neighbors, linear regression, or k-means clustering). The model is assuming that the data and features you're giving it are related in a linear fashion, or can be measured with a linear distance metric. "))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li",start:2},(0,i.kt)("li",{parentName:"ol"},"When a feature or features in your dataset have high variance ; if a feature in your dataset has a variance that's an order of magnitude or more greater than the other features, this could impact the model's ability to learn from other features in the dataset. "))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li",start:3},(0,i.kt)("li",{parentName:"ol"},"When features are of different scales e.g. height & weight. To compare these features, they must be in the same linear space, and therefore must be standardized in some way. "))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"StandardScaler"),": For Data Standardization",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Finds mean and centers data around it (no limit to max / min)"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Import"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.preprocessing import StandardScaler")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Creation"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"ss = StandardScaler()")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Fit"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"ss.fit(training_df_column)")," (Call before transform)",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"only fit with training data to avoid data leakage (won't have access to test data)"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Normalize scale"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"ss.transform(dataframe_subset (columns))"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"transform training data"),(0,i.kt)("li",{parentName:"ul"},"Use ",(0,i.kt)("inlineCode",{parentName:"li"},".fit_transform(...)")," to fit, then transform data"))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"scale"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Same purpose as standardscalar"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Import"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.preprocessing import scale")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Usage"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"scaled_data = scale(data)")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"MinMaxScaling"),": For normalizing linear values to 0-1 by squashing min and max range to 0-1",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Use only when you know your data has a strict lower and upper bound"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Import"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.preprocessing import MinMaxScaler")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Creation"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"MM_scaler = MinMaxScaler()")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Fit"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"MM_scaler.fit(training_df_column)")," (Call before transform)",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"only fit with training data"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Transform"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"resultantCol = MM_scaler.transform(training_df_column)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"transform training data"),(0,i.kt)("li",{parentName:"ul"},"Use ",(0,i.kt)("inlineCode",{parentName:"li"},".fit_transform(...)")," to fit, then transform data"))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Log transform"),": ",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Import"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.preprocessing import PowerTransformer")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Creation"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"pow_trans = PowerTransformer()")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Fit"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"pow_trans.fit(training_df_column)")," (Call before transform)",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"only fit with training data"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Normalize scale"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"pow_trans.transform(dataframe_subset (columns))"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"transform training data"),(0,i.kt)("li",{parentName:"ul"},"Use ",(0,i.kt)("inlineCode",{parentName:"li"},".fit_transform(...)")," to fit, then transform data")))))),(0,i.kt)("p",null,"Data Sanitization"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Outlier Removal",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Quantile (percentage) based",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li"},(0,i.kt)("li",{parentName:"ol"},"Find quantile: ",(0,i.kt)("inlineCode",{parentName:"li"},"quantile = dataframe['col'].quantile(0.95)")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li",start:2},(0,i.kt)("li",{parentName:"ol"},"Trim: ",(0,i.kt)("inlineCode",{parentName:"li"},"trimmed_df = dataframe[dataframe['col'] < quantile]")))))),(0,i.kt)("li",{parentName:"ul"},"standard dev based",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Get mean and std dev"),(0,i.kt)("li",{parentName:"ul"},"Calculate cutoff e.g. ",(0,i.kt)("inlineCode",{parentName:"li"},"3 * std"),", and lower (",(0,i.kt)("inlineCode",{parentName:"li"},"mean - cutoff"),") + upper (",(0,i.kt)("inlineCode",{parentName:"li"},"mean + cutoff"),") bounds"),(0,i.kt)("li",{parentName:"ul"},"Trim outliers ",(0,i.kt)("inlineCode",{parentName:"li"},"df[(df['col'] < upper) & (df['col'] > lower)]"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://towardsdatascience.com/5-ways-to-detect-outliers-that-every-data-scientist-should-know-python-code-70a54335a623"},"Detecting outliers")))))))),(0,i.kt)("li",{parentName:"ul"},"Text preprocessing tricks",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Trim whitespace"),(0,i.kt)("li",{parentName:"ul"},"Remove punctuation"),(0,i.kt)("li",{parentName:"ul"},"Remove commonly occurring words or stopwords"),(0,i.kt)("li",{parentName:"ul"},"Expanding contracted words (e.g. can't)"),(0,i.kt)("li",{parentName:"ul"},"Remove special characters such as numbers and emojis."),(0,i.kt)("li",{parentName:"ul"},"Done using ",(0,i.kt)("strong",{parentName:"li"},"tokenization")," (split corpus by space)"),(0,i.kt)("li",{parentName:"ul"},"Done by ",(0,i.kt)("strong",{parentName:"li"},"lemmatization"),' (convert word into base form e.g. "eating" "ate" into "eat")')))),(0,i.kt)("p",null,"Feature Engineering"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Descript: Creation of new features from existing features (e.g. string/timestamp subsetting, aggregate numeric data across columns etc)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"LabelEncoder"),": Converting labelled column into {0,1} column"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Creation"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"enc = LabelEncoder()")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Encode as binary"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"new_col = enc.fit_transform(dataframe['col'])")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"One Hot Encoding"),": Convert column of categories into 2D binary array"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Do this instead of mapping categories to numbers, to avoid encoding ordering to the categories"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Dummy Encoding"),": Same as one-hot encoding, but the first column dropped. Membership to the first column is indicated (implied) when all OHE are set to 0."),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Both OHE & Dummy use the same command:"))),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"df_dummied = pd.get_dummies(dataframe,\n    columns = [column_labels], \n    drop_first=True, \n    prefix='col_prefix'\n)\n"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"CountVectorizer"),": Vectorize text with word count"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Bag of words (BoW): Given a vocabulary e.g. ","['a','sock','dog']",", convert corpus into a dictionary counting number of times the words occur e.g. ","[0, 2, 1]"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Import"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.feature_extraction.text import CountVectorizer")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Creation"),": ")),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},'cv = CountVectorizer(\n    lowercase = bool,\n    stop_words = \'english\' # optional for excluding stop words in the BoW vector\n    min_df: 0-1,   # Use only words that occur in more than this percentage of documents. This can be used to remove outlier words that will not generalize across texts.\n    max_df: 0-1,   # Use only words that occur in less than this percentage of documents. This is useful to eliminate very common words that occur in every corpus without adding value such as "and" or "the".\n    ngram_range = (start<min=1>, stop) # don\'t forget curse of dimensionality\n)\n')),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Fit"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"cv.fit(speech_df['text_clean'])")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Get word list"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"cv.get_feature_names()")," "),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Transform"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"cv_transformed = cv.transform(speech_df['text_clean'])"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"returns sparse array"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"To array"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"cv_transformed.toarray()"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"1 row per block of text and a column for each of the features generated by the vectorizer"))))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"tf-idf"),": Term Frequency - Inverse Document Frequency vector"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Descript: Text vectorization converts text to numerical input"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("span",{parentName:"li",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"w"),(0,i.kt)("mrow",{parentName:"msub"},(0,i.kt)("mi",{parentName:"mrow"},"i"),(0,i.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,i.kt)("mi",{parentName:"mrow"},"j"))),(0,i.kt)("mo",{parentName:"mrow"},"="),(0,i.kt)("mi",{parentName:"mrow"},"t"),(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"f"),(0,i.kt)("mrow",{parentName:"msub"},(0,i.kt)("mi",{parentName:"mrow"},"i"),(0,i.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,i.kt)("mi",{parentName:"mrow"},"j"))),(0,i.kt)("mo",{parentName:"mrow"},"\u22c5"),(0,i.kt)("mi",{parentName:"mrow"},"l"),(0,i.kt)("mi",{parentName:"mrow"},"o"),(0,i.kt)("mi",{parentName:"mrow"},"g"),(0,i.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,i.kt)("mfrac",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"mfrac"},"N"),(0,i.kt)("mrow",{parentName:"mfrac"},(0,i.kt)("mi",{parentName:"mrow"},"d"),(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"f"),(0,i.kt)("mi",{parentName:"msub"},"i")))),(0,i.kt)("mo",{parentName:"mrow",stretchy:"false"},")")),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"w_{i,j} = tf_{i,j} \\cdot log(\\frac{N}{df_i})")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.716668em",verticalAlign:"-0.286108em"}}),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.02691em"}},"w"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.311664em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"-0.02691em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"i"),(0,i.kt)("span",{parentName:"span",className:"mpunct mtight"},","),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal mtight",style:{marginRight:"0.05724em"}},"j"))))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.286108em"}},(0,i.kt)("span",{parentName:"span"})))))),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2777777777777778em"}}),(0,i.kt)("span",{parentName:"span",className:"mrel"},"="),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2777777777777778em"}})),(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.980548em",verticalAlign:"-0.286108em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal"},"t"),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.10764em"}},"f"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.311664em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"-0.10764em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"i"),(0,i.kt)("span",{parentName:"span",className:"mpunct mtight"},","),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal mtight",style:{marginRight:"0.05724em"}},"j"))))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.286108em"}},(0,i.kt)("span",{parentName:"span"})))))),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}}),(0,i.kt)("span",{parentName:"span",className:"mbin"},"\u22c5"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}})),(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"1.3534389999999998em",verticalAlign:"-0.481108em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.01968em"}},"l"),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal"},"o"),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"g"),(0,i.kt)("span",{parentName:"span",className:"mopen"},"("),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mopen nulldelimiter"}),(0,i.kt)("span",{parentName:"span",className:"mfrac"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.872331em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.6550000000000002em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"d"),(0,i.kt)("span",{parentName:"span",className:"mord mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal mtight",style:{marginRight:"0.10764em"}},"f"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3280857142857143em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.357em",marginLeft:"-0.10764em",marginRight:"0.07142857142857144em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.5em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size3 size1 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"i")))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.143em"}},(0,i.kt)("span",{parentName:"span"}))))))))),(0,i.kt)("span",{parentName:"span",style:{top:"-3.23em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,i.kt)("span",{parentName:"span",className:"frac-line",style:{borderBottomWidth:"0.04em"}})),(0,i.kt)("span",{parentName:"span",style:{top:"-3.394em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal mtight",style:{marginRight:"0.10903em"}},"N"))))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.481108em"}},(0,i.kt)("span",{parentName:"span"}))))),(0,i.kt)("span",{parentName:"span",className:"mclose nulldelimiter"})),(0,i.kt)("span",{parentName:"span",className:"mclose"},")"))))),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("span",{parentName:"li",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"w"),(0,i.kt)("mrow",{parentName:"msub"},(0,i.kt)("mi",{parentName:"mrow"},"i"),(0,i.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,i.kt)("mi",{parentName:"mrow"},"j")))),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"w_{i,j}")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.716668em",verticalAlign:"-0.286108em"}}),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.02691em"}},"w"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.311664em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"-0.02691em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"i"),(0,i.kt)("span",{parentName:"span",className:"mpunct mtight"},","),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal mtight",style:{marginRight:"0.05724em"}},"j"))))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.286108em"}},(0,i.kt)("span",{parentName:"span"})))))))))),": weight of term i in document j"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("span",{parentName:"li",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("mi",{parentName:"mrow"},"t"),(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"f"),(0,i.kt)("mrow",{parentName:"msub"},(0,i.kt)("mi",{parentName:"mrow"},"i"),(0,i.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,i.kt)("mi",{parentName:"mrow"},"j")))),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"tf_{i,j}")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.980548em",verticalAlign:"-0.286108em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal"},"t"),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.10764em"}},"f"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.311664em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"-0.10764em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"i"),(0,i.kt)("span",{parentName:"span",className:"mpunct mtight"},","),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal mtight",style:{marginRight:"0.05724em"}},"j"))))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.286108em"}},(0,i.kt)("span",{parentName:"span"})))))))))),": term frequency of i in j"),(0,i.kt)("li",{parentName:"ul"},"N: # of documents in corpus"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("span",{parentName:"li",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("mi",{parentName:"mrow"},"d"),(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"f"),(0,i.kt)("mi",{parentName:"msub"},"i"))),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"df_i")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.8888799999999999em",verticalAlign:"-0.19444em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal"},"d"),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.10764em"}},"f"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.31166399999999994em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"-0.10764em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"i")))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,i.kt)("span",{parentName:"span"})))))))))),": # of documents containing term i"))),(0,i.kt)("li",{parentName:"ul"},"[(# of word occurences) / (total words in document)]"," / log(# of docs word is in / total number of docs)",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Divide proportion of word occurence by proportion that it appears in all documents",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"reduces value of common words"))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Import"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.feature_extraction.text import TfidfVectorizer")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Creation"),": ")),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},'tfidf_vec = TfidfVectorizer(\n  max_features=n, \n  stop_words=\'english\',\n  ngram_range = (start, stop) # Adding some ordering. bi-gram ("not happy") tri-gram ("never not happy"). Tries to fix bag of words problem\n)\n')),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"stop_words will remove common words like 'the'")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Transform"),": ",(0,i.kt)("inlineCode",{parentName:"p"},"text_tfidf = tfidf_vec.fit_transform(title_text)")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"returns sparse array"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"To array"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"cv_transformed.toarray()")),(0,i.kt)("li",{parentName:"ul"},"Indices and weights will be stored in the tfidf vector ",(0,i.kt)("inlineCode",{parentName:"li"},"text_tfidf")),(0,i.kt)("li",{parentName:"ul"},"Vocabulary and weights will be stored in the TfidfVectorizer ",(0,i.kt)("inlineCode",{parentName:"li"},"tfidf_vec"),"."),(0,i.kt)("li",{parentName:"ul"},"Example")),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"# Instantiate TfidfVectorizer\ntv = TfidfVectorizer(max_features=100, stop_words='english')\n\n# Fit the vectroizer and transform the data\ntv_transformed = tv.fit_transform(train_speech_df['text_clean'])\n\n# Transform test data\ntest_tv_transformed = tv.transform(test_speech_df['text_clean'])\n\n# Create new features for the test set\ntest_tv_df = pd.DataFrame(test_tv_transformed.toarray(), \n                          columns=tv.get_feature_names()).add_prefix('TFIDF_')\nprint(test_tv_df.head())\n")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Get word list"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"cv.get_feature_names()")," "),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Get word to index map"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"str_idx_map = tfidf_vec.vocabulary_"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Get index to word map"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"idx_str_map = {v:k for k,v in tfidf_vec.vocabulary_.items()}")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Get weights for every word in row"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"text_tfidf[row_index].data"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"If row_index is not number, use ",(0,i.kt)("inlineCode",{parentName:"li"},"iloc[n]")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Get indices for every word in row"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"text_tfidf[row_index].indices"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Get indices to weights map for row"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"idx_wgts_map = dict( zip(text_tfidf[row_index].indices, text_tfidf[row_index].data)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Get word to weights map for row"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"str_wgts_map = {idx_str_map[i]:idx_wgts_map[i] for i in text_tfidf[row_index].indices}")))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Get indices of top n weights"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"top_n_idx = pd.Series(str_wgts_map).sort_values(ascending=False)[:top_n].index")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Get words of top n weights"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"top_n_str = [idx_str_map[i] for i in top_n_idx]")))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Word Embeddings: Comparing not just the word frequencies, but the meanings of each word"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Use spacy")))),(0,i.kt)("p",null,"Feature Selection"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Removing ",(0,i.kt)("strong",{parentName:"li"},"redundant")," (noisy|correlated|duplicated) features.",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"e.g. features that generated an aggregate statistic"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"t-SNE"),": t-Distributed Stochastic Neighbour Embedding. ",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Used to significantly reeduce dimensions of a dataset, so you can visually explore the patterns in a high dimensional dataset."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.manifold import TSNE")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Init"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"m = TSNE(learn_rate_eg_50)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Fit & Transform"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"tsne_features = m.fit_transform(df_numeric)")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"To avoid overfitting"),': "number of observations should increase exponentially with the number of features"',(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Too few observations: model overfits by memorizing the test data"),(0,i.kt)("li",{parentName:"ul"},"Curse of dimensionality: when the number of observations is too high to satisfy")))),(0,i.kt)("p",null,"Feature Extraction"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Transform data into new features"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Dimensionality Reduction"),": Reduce feature space",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Turning numericals into binned vals / binary"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Feature selection:")," Drop irrelevant features"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li"},(0,i.kt)("li",{parentName:"ol"},"Remove redundant features (columns) where variance too small")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li"},(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.feature_selection import VarianceThreshold")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li",start:2},(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("inlineCode",{parentName:"li"},"sel = VarianceThreshold(threshold=0.005)")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li",start:3},(0,i.kt)("li",{parentName:"ol"},"Normalize variance for all columns: ",(0,i.kt)("inlineCode",{parentName:"li"},"norm_df = df / df.mean()")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li",start:4},(0,i.kt)("li",{parentName:"ol"},"Fit VarianceThreshold: ",(0,i.kt)("inlineCode",{parentName:"li"},"sel.fit(norm_df)")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li",start:5},(0,i.kt)("li",{parentName:"ol"},"Get mask: ",(0,i.kt)("inlineCode",{parentName:"li"},"mask = sel.get_support()")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li",start:6},(0,i.kt)("li",{parentName:"ol"},"Get columns with variance at least 0.005: ",(0,i.kt)("inlineCode",{parentName:"li"},"df.loc[:, mask]"))),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"0.005 not fixed; check the variances of your features before truncating"))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li",start:2},(0,i.kt)("li",{parentName:"ol"},"Remove features that are strongly correlated or duplicated")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li"},(0,i.kt)("li",{parentName:"ol"},"Get corr matrix (abs): ",(0,i.kt)("inlineCode",{parentName:"li"},"corr = df.corr().abs()")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li",start:2},(0,i.kt)("li",{parentName:"ol"},"Create True/False mask for upper triangle: ",(0,i.kt)("inlineCode",{parentName:"li"},"mask = np.triu(np.ones_like(corr, dtype=bool))")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li",start:3},(0,i.kt)("li",{parentName:"ol"},"Visualize with mask: ",(0,i.kt)("inlineCode",{parentName:"li"},'sns.heatmap(df.corr(), mask=mask, cmap=cmap, center=0, linewidths=1, annot=True, fmt=".2f"); plt.show()')))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li",start:4},(0,i.kt)("li",{parentName:"ol"},"Apply T/F mask; replace sections that are True with NAN: ",(0,i.kt)("inlineCode",{parentName:"li"},"tri_df = corr_matrix.mask(mask)")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li",start:5},(0,i.kt)("li",{parentName:"ol"},"Get cols that are highly correlated: ",(0,i.kt)("inlineCode",{parentName:"li"},"to_drop = [c for c in tri_df.columns if any(tri_df[c] >  0.95)]")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li",start:6},(0,i.kt)("li",{parentName:"ol"},"Drop: ",(0,i.kt)("inlineCode",{parentName:"li"},"reduced_df = df.drop(to_drop, axis=1)")))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li",start:3},(0,i.kt)("li",{parentName:"ol"},"Remove features with too many missing values")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"mask = school_df.isna().sum() / len(school_df) < 0.5")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"reduced_df = df.loc[:, mask]")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li",start:4},(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Recursive Feature Elimination"),": Remove features that have little effect on prediction (by seeing its weights)")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.feature_selection import RFE")),(0,i.kt)("li",{parentName:"ul"},"Init: ",(0,i.kt)("inlineCode",{parentName:"li"},"rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1, step=n_features_to_drop_per_iter)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Other estimators: ",(0,i.kt)("inlineCode",{parentName:"li"},"RandomForestClassifier(), GradientBoostingRegressor()")))),(0,i.kt)("li",{parentName:"ul"},"Fit: ",(0,i.kt)("inlineCode",{parentName:"li"},"rfe.fit(X_train, y_train)")),(0,i.kt)("li",{parentName:"ul"},"Columns ranked (highest = weakest): ",(0,i.kt)("inlineCode",{parentName:"li"},"print(dict(zip(X.columns, rfe.ranking_)))")),(0,i.kt)("li",{parentName:"ul"},"Columns kept (",(0,i.kt)("inlineCode",{parentName:"li"},"rfe.support_")," is a mask): ",(0,i.kt)("inlineCode",{parentName:"li"},"print(X.columns[rfe.support_])")),(0,i.kt)("li",{parentName:"ul"},"Test (using accuracy_score): ",(0,i.kt)("inlineCode",{parentName:"li"},"accuracy_score(y_test, rfe.predict(X_test))")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Feature Extraction:")," Combine features"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ol",{parentName:"li"},(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Principle Component Analysis (PCA)"),": linear transformation to uncorrelated space")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("img",{src:t(4694).Z,width:"287",height:"270"})),(0,i.kt)("li",{parentName:"ul"},"Change basis: 1 to represent variance (yellow vector), 1 to represent magnitude (red vector). Can be used to represent each point. Coordinates in this new reference system are called ",(0,i.kt)("strong",{parentName:"li"},"principle components"),"."),(0,i.kt)("li",{parentName:"ul"},"Vectors ranked by importance based on how well they explain variance (e.g.the red one would be most important as it best explains the variance)"),(0,i.kt)("li",{parentName:"ul"},"Variance captured in a meaningful way by combining features into components"),(0,i.kt)("li",{parentName:"ul"},"Number of components = number of input features"),(0,i.kt)("li",{parentName:"ul"},"Difficult to interpret components, should be left to the end-of-preprocessing journey"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Creation"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"pca = PCA(n_components=num_or_variance_ratio)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Transform"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"transformed_X = pca.fit_transform(dataframe_X)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Revert transformation"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"reverted_data = pca.inverse_transform(transformed_X)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Good for compressing / decompressing information, but lossy"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Components")," (vectors of weightage per feature): ",(0,i.kt)("inlineCode",{parentName:"li"},"pca.components_")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"How much variance is explained by each component"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"pca.explained_variance_ratio_")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Tagging component to column"),": ")),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"vectors = pipe.steps[1][1].components_.round(2)\n# Print feature effects\nprint('PC 1 effects = ' + str(dict(zip(poke_df.columns, vectors[0]))))\n"))))),(0,i.kt)("li",{parentName:"ul"},"POS tagging (Parts of Speech) (e.g. Pronoun, verb, article, noun)"),(0,i.kt)("li",{parentName:"ul"},"Named Entity Recognition (NER) (e.g. Person, Organization)")),(0,i.kt)("p",null,"Models"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"K-Nearest Neighbour (KNN)")," Classifier. ",(0,i.kt)("a",{parentName:"p",href:"https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn"},"See")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"See also ",(0,i.kt)("a",{parentName:"li",href:"https://www.analyticsvidhya.com/blog/2018/08/k-nearest-neighbor-introduction-regression-python/"},"what")," and ",(0,i.kt)("a",{parentName:"li",href:"https://stats.stackexchange.com/questions/104255/why-would-anyone-use-knn-for-regression"},"why")," and ",(0,i.kt)("a",{parentName:"li",href:"https://www.fromthegenesis.com/pros-and-cons-of-k-nearest-neighbors/"},"pros/cons")),(0,i.kt)("li",{parentName:"ul"},"Return label of closest neighbour as prediction"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Import"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.neighbors import KNeighborsClassifier")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Create"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"knn = KNeighborsClassifier(n_neighbors=3)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"You can iterate through the # of neighbors to find the best fit"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Training"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"knn.fit(X_train (features), y_train (labels))")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Scoring"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"knn.score(X_test, y_test)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Predict"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"predicted = knn.predict(input)"))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Naive Bayes")," Classifier: "),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Assumes features are independent, works well with high-dimensional text data"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Training"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"nb.fit(X_train (features), y_train (labels))")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Scoring"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"nb.score(X_test, y_test)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Predict"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"predicted = nb.predict(input)"))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Multinomial Naive Bayes")," classifier (0 or 1):"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Init: ",(0,i.kt)("inlineCode",{parentName:"li"},"clf = MultinomialNB()")),(0,i.kt)("li",{parentName:"ul"},"Fit: ",(0,i.kt)("inlineCode",{parentName:"li"},"clf.fit(X_train_BoW, y_train)")),(0,i.kt)("li",{parentName:"ul"},"Scoring: ",(0,i.kt)("inlineCode",{parentName:"li"},"accuracy = clf.score(X_test_bow, y_test)")),(0,i.kt)("li",{parentName:"ul"},"Prediction example: ")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'# Predict the sentiment of a negative review\nreview = "The movie was terrible. The music was underwhelming and the acting mediocre."\nprediction = clf.predict(vectorizer.transform([review]))[0]\n')),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Support Vector Classifier")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.svm import SVC")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.metrics import accuracy_score")),(0,i.kt)("li",{parentName:"ul"},"Init: ",(0,i.kt)("inlineCode",{parentName:"li"},"svc = SVC()")),(0,i.kt)("li",{parentName:"ul"},"Fit: ",(0,i.kt)("inlineCode",{parentName:"li"},"svc.fit(X_train, y_train)")),(0,i.kt)("li",{parentName:"ul"},"Scoring: ",(0,i.kt)("inlineCode",{parentName:"li"},"accuracy = accuracy_score(y_test, svc.predict(X_test))")),(0,i.kt)("li",{parentName:"ul"},"Check overfit: ",(0,i.kt)("inlineCode",{parentName:"li"},"accuracy_train = accuracy_score(y_train, svc.predict(X_train))"))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Logistic Regression")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Features need to be scaled first using standardscaler"),(0,i.kt)("li",{parentName:"ul"},"Init: ",(0,i.kt)("inlineCode",{parentName:"li"},"lr = LogisticRegression()")),(0,i.kt)("li",{parentName:"ul"},"Fit: ",(0,i.kt)("inlineCode",{parentName:"li"},"lr.fit(X_train_std, y_train)")),(0,i.kt)("li",{parentName:"ul"},"Predict: ",(0,i.kt)("inlineCode",{parentName:"li"},"y_pred = lr.predict(X_test_std)")),(0,i.kt)("li",{parentName:"ul"},"Predict (probability of 1): ",(0,i.kt)("inlineCode",{parentName:"li"},"proba_1 = logreg.predict_proba(X_test_std)")),(0,i.kt)("li",{parentName:"ul"},"Weights: ",(0,i.kt)("inlineCode",{parentName:"li"},"print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Random Forest Classifier")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Algorithm to calculate feature importance by averaging how often features are used to make decisions in multiple random decision trees"),(0,i.kt)("li",{parentName:"ul"},"Init: ",(0,i.kt)("inlineCode",{parentName:"li"},"rf = RandomForestClassifier()")),(0,i.kt)("li",{parentName:"ul"},"Fit: ",(0,i.kt)("inlineCode",{parentName:"li"},"rf.fit(X_train, y_train)")),(0,i.kt)("li",{parentName:"ul"},"Predict Accuracy: ",(0,i.kt)("inlineCode",{parentName:"li"},"acc = accuracy_score(y_test, rf.predict(X_test))")),(0,i.kt)("li",{parentName:"ul"},"Get feature importances mask: ",(0,i.kt)("inlineCode",{parentName:"li"},"mask = rf.feature_importances_ > 0.1"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Select by mask: ",(0,i.kt)("inlineCode",{parentName:"li"},"df.loc(:, mask)")))),(0,i.kt)("li",{parentName:"ul"},"Use with RFE: `RFE(estimator=RandomForestClassifier(), ...)")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Linear Regression")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("span",{parentName:"li",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("mi",{parentName:"mrow"},"y"),(0,i.kt)("mo",{parentName:"mrow"},"="),(0,i.kt)("mi",{parentName:"mrow"},"c"),(0,i.kt)("mo",{parentName:"mrow"},"+"),(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"w"),(0,i.kt)("mn",{parentName:"msub"},"1")),(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"x"),(0,i.kt)("mn",{parentName:"msub"},"1")),(0,i.kt)("mo",{parentName:"mrow"},"+"),(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"w"),(0,i.kt)("mn",{parentName:"msub"},"2")),(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"x"),(0,i.kt)("mn",{parentName:"msub"},"2")),(0,i.kt)("mo",{parentName:"mrow"},"+"),(0,i.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"."),(0,i.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"."),(0,i.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"."),(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"w"),(0,i.kt)("mi",{parentName:"msub"},"n")),(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"x"),(0,i.kt)("mi",{parentName:"msub"},"n"))),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"y = c + w_1x_1 + w_2x_2 + ... w_nx_n")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.625em",verticalAlign:"-0.19444em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"y"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2777777777777778em"}}),(0,i.kt)("span",{parentName:"span",className:"mrel"},"="),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2777777777777778em"}})),(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.66666em",verticalAlign:"-0.08333em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal"},"c"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}}),(0,i.kt)("span",{parentName:"span",className:"mbin"},"+"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}})),(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.73333em",verticalAlign:"-0.15em"}}),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.02691em"}},"w"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.30110799999999993em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"-0.02691em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mtight"},"1")))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,i.kt)("span",{parentName:"span"})))))),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal"},"x"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.30110799999999993em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"0em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mtight"},"1")))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,i.kt)("span",{parentName:"span"})))))),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}}),(0,i.kt)("span",{parentName:"span",className:"mbin"},"+"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}})),(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.73333em",verticalAlign:"-0.15em"}}),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.02691em"}},"w"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.30110799999999993em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"-0.02691em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mtight"},"2")))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,i.kt)("span",{parentName:"span"})))))),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal"},"x"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.30110799999999993em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"0em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mtight"},"2")))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,i.kt)("span",{parentName:"span"})))))),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}}),(0,i.kt)("span",{parentName:"span",className:"mbin"},"+"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}})),(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.58056em",verticalAlign:"-0.15em"}}),(0,i.kt)("span",{parentName:"span",className:"mord"},"."),(0,i.kt)("span",{parentName:"span",className:"mord"},"."),(0,i.kt)("span",{parentName:"span",className:"mord"},"."),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.02691em"}},"w"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.151392em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"-0.02691em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"n")))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,i.kt)("span",{parentName:"span"})))))),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal"},"x"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.151392em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"0em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"n")))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,i.kt)("span",{parentName:"span"}))))))))))," + error"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.linear_model import LinearRegression")),(0,i.kt)("li",{parentName:"ul"},"init: ",(0,i.kt)("inlineCode",{parentName:"li"},"lr = LinearRegression")),(0,i.kt)("li",{parentName:"ul"},"fit: ",(0,i.kt)("inlineCode",{parentName:"li"},"lr.fit(X_train, y_train)")),(0,i.kt)("li",{parentName:"ul"},"coefficients (",(0,i.kt)("span",{parentName:"li",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"w"),(0,i.kt)("mi",{parentName:"msub"},"i"))),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"w_i")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.58056em",verticalAlign:"-0.15em"}}),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.02691em"}},"w"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.31166399999999994em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"-0.02691em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"i")))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,i.kt)("span",{parentName:"span"})))))))))),"): ",(0,i.kt)("inlineCode",{parentName:"li"},"lr.coef_")),(0,i.kt)("li",{parentName:"ul"},"How it works:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Minimize loss function: Mean Squared Error (MSE)"),(0,i.kt)("li",{parentName:"ul"},"Avoid overfitting by using ",(0,i.kt)("strong",{parentName:"li"},"regularization"),": goal is to keep model simple by keeping coefficients low",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Regularization term: ",(0,i.kt)("span",{parentName:"li",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("mi",{parentName:"mrow"},"\u03b1"),(0,i.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,i.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"\u2223"),(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"\u03b2"),(0,i.kt)("mn",{parentName:"msub"},"1")),(0,i.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"\u2223"),(0,i.kt)("mo",{parentName:"mrow"},"+"),(0,i.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"\u2223"),(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"\u03b2"),(0,i.kt)("mn",{parentName:"msub"},"2")),(0,i.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"\u2223"),(0,i.kt)("mo",{parentName:"mrow"},"+"),(0,i.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"."),(0,i.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"."),(0,i.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"."),(0,i.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"\u2223"),(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"\u03b2"),(0,i.kt)("mi",{parentName:"msub"},"n")),(0,i.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"\u2223"),(0,i.kt)("mo",{parentName:"mrow",stretchy:"false"},")")),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"\\alpha (|\\beta_1| + |\\beta_2| + ... |\\beta_n|)")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.0037em"}},"\u03b1"),(0,i.kt)("span",{parentName:"span",className:"mopen"},"("),(0,i.kt)("span",{parentName:"span",className:"mord"},"\u2223"),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.05278em"}},"\u03b2"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.30110799999999993em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"-0.05278em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mtight"},"1")))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,i.kt)("span",{parentName:"span"})))))),(0,i.kt)("span",{parentName:"span",className:"mord"},"\u2223"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}}),(0,i.kt)("span",{parentName:"span",className:"mbin"},"+"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}})),(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.kt)("span",{parentName:"span",className:"mord"},"\u2223"),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.05278em"}},"\u03b2"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.30110799999999993em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"-0.05278em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mtight"},"2")))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,i.kt)("span",{parentName:"span"})))))),(0,i.kt)("span",{parentName:"span",className:"mord"},"\u2223"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}}),(0,i.kt)("span",{parentName:"span",className:"mbin"},"+"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}})),(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.kt)("span",{parentName:"span",className:"mord"},"."),(0,i.kt)("span",{parentName:"span",className:"mord"},"."),(0,i.kt)("span",{parentName:"span",className:"mord"},"."),(0,i.kt)("span",{parentName:"span",className:"mord"},"\u2223"),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.05278em"}},"\u03b2"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.151392em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"-0.05278em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"n")))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,i.kt)("span",{parentName:"span"})))))),(0,i.kt)("span",{parentName:"span",className:"mord"},"\u2223"),(0,i.kt)("span",{parentName:"span",className:"mclose"},")")))))),(0,i.kt)("li",{parentName:"ul"},"Alpha: trade-off between overfitting and keeping model simple. Too low = overfit, Too high = inaccurate model"),(0,i.kt)("li",{parentName:"ul"},"Final regularized expression: ",(0,i.kt)("span",{parentName:"li",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("mi",{parentName:"mrow"},"y"),(0,i.kt)("mo",{parentName:"mrow"},"="),(0,i.kt)("mi",{parentName:"mrow"},"c"),(0,i.kt)("mo",{parentName:"mrow"},"+"),(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"w"),(0,i.kt)("mn",{parentName:"msub"},"1")),(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"x"),(0,i.kt)("mn",{parentName:"msub"},"1")),(0,i.kt)("mo",{parentName:"mrow"},"+"),(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"w"),(0,i.kt)("mn",{parentName:"msub"},"2")),(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"x"),(0,i.kt)("mn",{parentName:"msub"},"2")),(0,i.kt)("mo",{parentName:"mrow"},"+"),(0,i.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"."),(0,i.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"."),(0,i.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"."),(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"w"),(0,i.kt)("mi",{parentName:"msub"},"n")),(0,i.kt)("msub",{parentName:"mrow"},(0,i.kt)("mi",{parentName:"msub"},"x"),(0,i.kt)("mi",{parentName:"msub"},"n"))),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"y = c + w_1x_1 + w_2x_2 + ... w_nx_n")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.625em",verticalAlign:"-0.19444em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"y"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2777777777777778em"}}),(0,i.kt)("span",{parentName:"span",className:"mrel"},"="),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2777777777777778em"}})),(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.66666em",verticalAlign:"-0.08333em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal"},"c"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}}),(0,i.kt)("span",{parentName:"span",className:"mbin"},"+"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}})),(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.73333em",verticalAlign:"-0.15em"}}),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.02691em"}},"w"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.30110799999999993em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"-0.02691em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mtight"},"1")))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,i.kt)("span",{parentName:"span"})))))),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal"},"x"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.30110799999999993em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"0em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mtight"},"1")))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,i.kt)("span",{parentName:"span"})))))),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}}),(0,i.kt)("span",{parentName:"span",className:"mbin"},"+"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}})),(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.73333em",verticalAlign:"-0.15em"}}),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.02691em"}},"w"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.30110799999999993em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"-0.02691em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mtight"},"2")))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,i.kt)("span",{parentName:"span"})))))),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal"},"x"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.30110799999999993em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"0em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mtight"},"2")))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,i.kt)("span",{parentName:"span"})))))),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}}),(0,i.kt)("span",{parentName:"span",className:"mbin"},"+"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}})),(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.58056em",verticalAlign:"-0.15em"}}),(0,i.kt)("span",{parentName:"span",className:"mord"},"."),(0,i.kt)("span",{parentName:"span",className:"mord"},"."),(0,i.kt)("span",{parentName:"span",className:"mord"},"."),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.02691em"}},"w"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.151392em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"-0.02691em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"n")))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,i.kt)("span",{parentName:"span"})))))),(0,i.kt)("span",{parentName:"span",className:"mord"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal"},"x"),(0,i.kt)("span",{parentName:"span",className:"msupsub"},(0,i.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.151392em"}},(0,i.kt)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"0em",marginRight:"0.05em"}},(0,i.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,i.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,i.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"n")))),(0,i.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,i.kt)("span",{parentName:"span",className:"vlist-r"},(0,i.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,i.kt)("span",{parentName:"span"}))))))))))," + error + regularization_term"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Lasso"),": 1 linear regression model that uses regularization"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.linear_model import Lasso")),(0,i.kt)("li",{parentName:"ul"},"Ensure X_data is standardized using StandardScaler"),(0,i.kt)("li",{parentName:"ul"},"Init: ",(0,i.kt)("inlineCode",{parentName:"li"},"la = Lasso(alpha={0.0-1.0}, random_state={0,1}, normalize=bool)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Recall that in regression regularization, alpha here refers to the weight of the regularization term"))),(0,i.kt)("li",{parentName:"ul"},"Fit: ",(0,i.kt)("inlineCode",{parentName:"li"},"la.fit(X_train_std, y_train)")),(0,i.kt)("li",{parentName:"ul"},"coeffs: ",(0,i.kt)("inlineCode",{parentName:"li"},"la.coef_ ")),(0,i.kt)("li",{parentName:"ul"},"score (R squared): ",(0,i.kt)("inlineCode",{parentName:"li"},"la.score(X_test_std, y_test)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"LassoCV"),": Cross Validation to find optimal alpha value (Same syntax as above)"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Ridge"),": Another regressor like Lasso, but supposedly better"),(0,i.kt)("li",{parentName:"ul"},"Init: Same syntax"),(0,i.kt)("li",{parentName:"ul"},"You can modify the alpha on the fly using ",(0,i.kt)("inlineCode",{parentName:"li"},"ridge.alpha = a")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.linear_model import Ridge"))))))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Confusion Matrix")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Generate table of ","[0,0]",": True Positive, ","[0,1]",": False Negative, ","[1,0]",": True Negative, ","[1,1]",": False Positive"),(0,i.kt)("li",{parentName:"ul"},"Import: ",(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.metrics import confusion_matrix")),(0,i.kt)("li",{parentName:"ul"},"Import: ",(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.metrics import classification_report")),(0,i.kt)("li",{parentName:"ul"},"Init: ")),(0,i.kt)("p",null,"Comparing similarity between vectors"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Cosine Similarity"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.metrics.pairwise import cosine_similarity")),(0,i.kt)("li",{parentName:"ul"},"Ratio of (dot product of vectors) and (product of magnitude of vectors)"),(0,i.kt)("li",{parentName:"ul"},"Range of values from -1 to 1, but since NLP vectors are non-neg, it's 0 - 1"),(0,i.kt)("li",{parentName:"ul"},"Calcualting ",(0,i.kt)("strong",{parentName:"li"},"pairwise")," cosine similarity matrix: ",(0,i.kt)("inlineCode",{parentName:"li"},"cosine_similarity(A, B)")," e.g. ",(0,i.kt)("inlineCode",{parentName:"li"},"cosine_similarity(tfidf_mat, tfidf_mat)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"A and B can be the same ",(0,i.kt)("inlineCode",{parentName:"li"},"tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)")," if you want to check similiarity across entries"))),(0,i.kt)("li",{parentName:"ul"},"When comparing tf-idf vectors, their magnitude is 1, so use ",(0,i.kt)("inlineCode",{parentName:"li"},"linear_kernel")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.metrics.pairwise import linear_kernel"))),(0,i.kt)("p",null,"Bootstrap"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Given a sample of data, resample from this data",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Bootstrap sample"),": array of resampled data"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Bootstrap replicate"),": summary statistics (e.g. mean) of this array of resampled data"),(0,i.kt)("li",{parentName:"ul"},"You can resample using ",(0,i.kt)("inlineCode",{parentName:"li"},"np.random.choice(..)"))))),(0,i.kt)("h2",{id:"verifying-model-accuracy"},"Verifying model accuracy"),(0,i.kt)("p",null,"n-fold Cross-validation"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Concept:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},'Split data into n chunks (referred to as "folds")'),(0,i.kt)("li",{parentName:"ul"},"Train on (n-1) chunks, test on the last chunk"),(0,i.kt)("li",{parentName:"ul"},"Repeat this procedure n times (refresh the model each time), each time testing with a different chunk"),(0,i.kt)("li",{parentName:"ul"},"Gather all the R^2 values from these tests to build mean, var and confidence interval of R^2"))),(0,i.kt)("li",{parentName:"ul"},"Import: ",(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.model_selection import cross_val_score")),(0,i.kt)("li",{parentName:"ul"},"Usage: ",(0,i.kt)("inlineCode",{parentName:"li"},"cv_scores_array = cross_val_score(LinearRegression(), X, y, cv=n)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"you can replace ",(0,i.kt)("inlineCode",{parentName:"li"},"LinearRegression()")," with other models such as ",(0,i.kt)("inlineCode",{parentName:"li"},"Ridge()")),(0,i.kt)("li",{parentName:"ul"},"you can also get the mean cv_scores by ",(0,i.kt)("inlineCode",{parentName:"li"},"np.mean()"))))),(0,i.kt)("p",null,"ROC curve"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"ROC: y: True Positive Rate / x: False Positive Rate"),(0,i.kt)("li",{parentName:"ul"},"Precision: TP/(TP+FP)"),(0,i.kt)("li",{parentName:"ul"},"Recall:    TP/(TP+FN)"),(0,i.kt)("li",{parentName:"ul"},"Import: ",(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.metrics import roc_curve"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.metrics import roc_auc_score")),(0,i.kt)("li",{parentName:"ul"},"Plotting ROC curve:")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"# Compute predicted probabilities: y_pred_prob\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\n\n# AUC: Area Under Curve\n# Compute and print AUC score\nprint(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n\n# Compute cross-validated AUC scores: cv_auc\ncv_auc = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc')\n\n# Print list of AUC scores\nprint(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))\n")),(0,i.kt)("h2",{id:"hyperparam-tuning"},"Hyperparam Tuning"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"GridSearchCV"),": iterate through all the possible hyperparams and then choosing the best"),(0,i.kt)("li",{parentName:"ul"},"import: ",(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.model_selection import GridSearchCV")),(0,i.kt)("li",{parentName:"ul"},"Init: ",(0,i.kt)("inlineCode",{parentName:"li"},"logreg_cv = GridSearchCV(LogisticRegression()/pipeline, param_grid, cv=5)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Hyperparam grid: ",(0,i.kt)("inlineCode",{parentName:"li"},"param_grid = {'stepName__param': np.logspace(-5, 8, 15)}")),(0,i.kt)("li",{parentName:"ul"},"There are many classifiers such as ElasticNet."))),(0,i.kt)("li",{parentName:"ul"},"Create hold-out set: ",(0,i.kt)("inlineCode",{parentName:"li"},"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4, random_state=42)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Hold-out set: split training/test set even before doing searchCV; makes sure that the model generalizes well to unknown data"))),(0,i.kt)("li",{parentName:"ul"},"Fit Searcher to data: ",(0,i.kt)("inlineCode",{parentName:"li"},"logreg_cv.fit(X_train,y_train)")),(0,i.kt)("li",{parentName:"ul"},"Get best params (classifier l1 ratio): ",(0,i.kt)("inlineCode",{parentName:"li"},"logreg_cv.best_params_")),(0,i.kt)("li",{parentName:"ul"},"Get score of best params: ",(0,i.kt)("inlineCode",{parentName:"li"},"logreg_cv.best_score_")),(0,i.kt)("li",{parentName:"ul"},"Get r2: ",(0,i.kt)("inlineCode",{parentName:"li"},"logreg_cv.score(X_test, y_test)")),(0,i.kt)("li",{parentName:"ul"},"Get MSE: ",(0,i.kt)("inlineCode",{parentName:"li"},"mse = mean_squared_error(y_test, logreg_cv.predict(X_test))")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"RandomSearchCV"),": same concept and syntax as above (fit, then best",(0,i.kt)("em",{parentName:"li"},"params"),"), for larger search spaces",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.model_selection import RandomizedSearchCV")),(0,i.kt)("li",{parentName:"ul"},"Init: ",(0,i.kt)("inlineCode",{parentName:"li"},"tree_cv = RandomizedSearchCV(DecisionTreeClassifier(), param_dist, cv=5)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},'param_dist = {"max_depth": [3, None],"max_features": randint(1, 9), "min_samples_leaf": randint(1, 9), "criterion": ["gini", "entropy"]}'))))))),(0,i.kt)("h3",{id:"imputer-impute-missing-info"},"Imputer (Impute missing info)"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"from sklearn.preprocessing import Imputer")),(0,i.kt)("li",{parentName:"ul"},"Purpose: preprocessing data"),(0,i.kt)("li",{parentName:"ul"},"Init: ",(0,i.kt)("inlineCode",{parentName:"li"},"Imputer(missing_values='NaN', strategy='most_frequent|mean', axis=0|1"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"axis: is_row "))),(0,i.kt)("li",{parentName:"ul"},"As part of pipeline: ",(0,i.kt)("inlineCode",{parentName:"li"},"steps = [('imputation', initialized_imputer),...]"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"pipe = Pipeline(steps)"))))),(0,i.kt)("h2",{id:"numpy"},"Numpy"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Import"),": ",(0,i.kt)("inlineCode",{parentName:"p"},"import numpy as np"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Numpy Array")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Creation",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Create array"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.array(list)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Create empty"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.empty(shape)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Create empty like"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.empty_like(list)")))),(0,i.kt)("li",{parentName:"ul"},"Attributes",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Shape"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np_list.shape")))),(0,i.kt)("li",{parentName:"ul"},"Unary Operations",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Transpose array"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.transpose(list)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Reshape"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.reshape(np_array, (new, shape, dimensions))"),". ",(0,i.kt)("a",{parentName:"li",href:"https://numpy.org/doc/stable/reference/generated/numpy.reshape.html"},"Doc")))),(0,i.kt)("li",{parentName:"ul"},"Operation with scalar",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Slicing 2D numpy array (get row / get col)"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np_2d[row,:]"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"np_2d[:,col]")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Mass multiplication"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np_list *= val")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Iterate 2D array as 1D"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"for x in np.nditer(np_2d_arr)")))),(0,i.kt)("li",{parentName:"ul"},"Operations involving two numpy arrays",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Mass operation"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np_list3 = np_list1 / np_list2")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Union"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.logical_and(arr1, arr2)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Intersect"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.logical_or(a1,a2)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Negation of numpy arr"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.logical_not(a1,a2)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Concat"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.concatenate([list1, list2,...])")))),(0,i.kt)("li",{parentName:"ul"},"Operations involving boolean arrays",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Generate boolean array by applying condition to array"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"bool_val = np_list > val")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Subsetting with bool array"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np_list[bool_val]")))),(0,i.kt)("li",{parentName:"ul"},"Aggregation Operations",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Mean"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.mean(np_list)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Median"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.median(np_list)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Std Dev"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.std(np_list)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Standard Error (of the mean)"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.std(data) / np.sqrt(len(data))"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},'"Standard deviation (SD) measures the dispersion of a dataset relative to its mean."'),(0,i.kt)("li",{parentName:"ul"},'"Standard error of the mean (SEM) measured how much discrepancy there is likely to be in a sample\'s mean compared to the population mean."'),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://www.investopedia.com/ask/answers/042415/what-difference-between-standard-error-means-and-standard-deviation.asp"},"See more")))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Variance"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.var(np_list)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Covariance Matrix"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.cov(np_list1, np_list2)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"[0,0]",": x.var(), ","[1,1]",": y.var(), ","[0,1]"," = ","[1,0]",": covariance"),(0,i.kt)("li",{parentName:"ul"},"mean of the distances from the mean (negative or positive, showing how correlated x and y are)"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("img",{src:t(5567).Z,width:"478",height:"400"})))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Pearson Correlation Coefficient"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.corrcef(np_list1, np_list2)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"[0,0]",": 1, ","[1,1]",": 1, ","[0,1]",": correlation coeff"))),(0,i.kt)("li",{parentName:"ul"},"covariance / (x.std * y.std)"),(0,i.kt)("li",{parentName:"ul"},"variability due to codependence / (interpendent variability)"))))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Numpy Random")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Init with Seed"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.random.seed(seed)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Generate Float")," 0-1: ",(0,i.kt)("inlineCode",{parentName:"li"}," np.random.random()")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Generate Integer"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.random.randint(lowerIncl, upperExcl)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Binomial"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.random.binomial(n, p, size=sample_size)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Poisson"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.random.poisson(mean, size=sample_size)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Exponential"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.random.exponential(mean, size=sample_size)"),"  "),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Normal Distribution"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.random.normal(mean, std, size=sample_size)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"tails are very unlikely; real datasets can have extreme outliers, normal distribution may not be best descriptor for data"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Permutate (shuffle)"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.random.permutate(list)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"If we can assume that two samples (e.g. Ohio and PA) share the same distribution, then we can ",(0,i.kt)("inlineCode",{parentName:"li"},"concatenate")," them, permutate (shuffle), then split the samples into the same sizes to get new samples. These are called ",(0,i.kt)("strong",{parentName:"li"},"permutation samples")))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Numpy Definitions")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"NaN"),": ",(0,i.kt)("inlineCode",{parentName:"li"},"np.nan")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Numpy Operations")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Cumulative Sum"),": ",(0,i.kt)("inlineCode",{parentName:"p"},"np.cumsum(list)"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Log10"),": ",(0,i.kt)("inlineCode",{parentName:"p"},"np.log10(nd_arr or dataframe)"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"1D interpolation"),": ",(0,i.kt)("a",{parentName:"p",href:"https://numpy.org/doc/stable/reference/generated/numpy.interp.html"},"doc"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Calculate Percentiles"),": ",(0,i.kt)("inlineCode",{parentName:"p"},"np.percentile(np_data, np_array_of_percentiles_zero_to_hundrd)")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"e.g. 95% confidence interval: ",(0,i.kt)("inlineCode",{parentName:"li"},"np.percentile(data, [2.5, 97.5])")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Confidence Interval"),": draw_bs_reps,then take percentile"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Generate range as list"),": ",(0,i.kt)("inlineCode",{parentName:"p"},"numpy.arange([start=0, ]stop, [step=1])"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"least squares"),": ",(0,i.kt)("inlineCode",{parentName:"p"},"slope, intercept = np.polyfit(x, y, deg_of_poly_to_fit<1>)"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Bootstrap samping (resampling with replacement)"),": ",(0,i.kt)("inlineCode",{parentName:"p"},"np.random.choice(arr_data, size=n)")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Imagine this as t-test but simulate by resampling ",(0,i.kt)("a",{parentName:"li",href:"https://stats.stackexchange.com/questions/128987/why-would-i-want-to-bootstrap-when-computing-an-independent-sample-t-test-how#:~:text=Both%20the%20t%2Dtest%20and,of%20the%20test%20statistic%20is.&text=The%20advantage%20of%20the%20bootstrap,assumptions%20needed%20by%20parametric%20methods."},"See also")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Pairs Bootstrap"),": resample with replacement x and y coords. that cannot be separated"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Hypothesis testing"),":"),(0,i.kt)("li",{parentName:"ul"},"Compute mean of combined data set, Shift the samples, Get bootstrap replicates of shifted data sets, Compute replicates of difference of means: bs_diff_replicates, Compute the p-value")),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},'# Helper functions used in datacamp\ndef bootstrap_replicate_1d(data, func):\n    """Generate bootstrap replicate of 1D data."""\n    bs_sample = np.random.choice(data, len(data))\n    return func(bs_sample)\n\ndef draw_bs_reps(data, func, size=1):\n    """Draw bootstrap replicates."""\n\n    # Initialize array of replicates: bs_replicates\n    bs_replicates = np.empty(size)\n\n    # Generate replicates\n    for i in range(size):\n        bs_replicates[i] = bootstrap_replicate_1d(data, func)\n\n    return bs_replicates\n\ndef draw_bs_pairs_linreg(x, y, size=1):\n    """Perform pairs bootstrap for linear regression."""\n\n    # Set up array of indices to sample from: inds\n    inds = np.arange(0, len(x))\n\n    # Initialize replicates: bs_slope_reps, bs_intercept_reps\n    bs_slope_reps = np.empty(size)\n    bs_intercept_reps = np.empty(size)\n\n    # Generate replicates\n    for i in range(size):\n        bs_inds = np.random.choice(inds, size=len(inds))\n        bs_x, bs_y = x[bs_inds], y[bs_inds]\n        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)\n\n    return bs_slope_reps, bs_intercept_reps\n\ndef draw_bs_pairs(x, y, func, size=1):\n    """Perform pairs bootstrap for a single statistic."""\n\n    # Set up array of indices to sample from: inds\n    inds = np.arange(len(x))\n\n    # Initialize replicates: bs_replicates\n    bs_replicates = np.empty(size)\n\n    # Generate replicates\n    for i in range(size):\n        bs_inds = np.random.choice(inds, len(inds))\n        bs_x, bs_y = x[bs_inds], y[bs_inds]\n        bs_replicates[i] = func(bs_x, bs_y)\n\n    return bs_replicates\n\ndef permutation_sample(data1, data2):\n    """Generate a permutation sample from two data sets."""\n\n    # Concatenate the data sets: data\n    data = np.concatenate([data1, data2])\n\n    # Permute the concatenated array: permuted_data\n    permuted_data = np.random.permutation(data)\n\n    # Split the permuted array into two: perm_sample_1, perm_sample_2\n    perm_sample_1 = permuted_data[:len(data1)]\n    perm_sample_2 = permuted_data[len(data1):]\n\n    return perm_sample_1, perm_sample_2\ndef draw_perm_reps(data_1, data_2, func, size=1):\n    """Generate multiple permutation replicates."""\n\n    # Initialize array of replicates: perm_replicates\n    perm_replicates = np.empty(size)\n\n    for i in range(size):\n        # Generate permutation sample\n        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)\n\n        # Compute the test statistic\n        perm_replicates[i] = func(perm_sample_1, perm_sample_2)\n\n    return perm_replicates\n\n# In null hypothesis significance testing, the p-value[note 1] is the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.\n# https://en.wikipedia.org/wiki/P-value\ndef p_value(new_samples, true_value):\n    return np.sum(new_samples <= true_value) / 10000\n\n# One-sample bootstrap hypothesis test example from datacamp\n# Make an array of translated impact forces: translated_force_b\ntranslated_force_b = force_b + (0.55 - force_b.mean())\n\n# Take bootstrap replicates of Frog B\'s translated impact forces: bs_replicates\nbs_replicates = draw_bs_reps(translated_force_b, np.mean, 10000)\n\n# Compute fraction of replicates that are less than the observed Frog B force: p\np = np.sum(bs_replicates <= np.mean(force_b)) / 10000\n\n# Print the p-value\nprint(\'p = \', p)\n\n# Two-sample bootstrap hypothesis test example from datacamp\n# Compute mean of all forces: mean_force\nmean_force = forces_concat.mean()\n\n# Generate shifted arrays\nforce_a_shifted = force_a - np.mean(force_a) + mean_force\nforce_b_shifted = force_b - np.mean(force_b) + mean_force \n\n# Compute 10,000 bootstrap replicates from shifted arrays\nbs_replicates_a = draw_bs_reps(force_a_shifted, np.mean, 10000)\nbs_replicates_b = draw_bs_reps(force_b_shifted, np.mean, 10000)\n\n# Get replicates of difference of means: bs_replicates\nbs_replicates = bs_replicates_a - bs_replicates_b\n\n# Compute and print p-value: p\np = np.sum(bs_replicates > empirical_diff_means) / 10000\nprint(\'p-value =\', p)\n'))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"A/B Testing"),": Hypothesis testing, but testing the difference"))),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"# Compute the observed difference in mean inter-no-hitter times: nht_diff_obs\nnht_diff_obs = diff_of_means(nht_dead, nht_live)\n\n# Acquire 10,000 permutation replicates of difference in mean no-hitter time: perm_replicates\nperm_replicates = draw_perm_reps(nht_dead, nht_live, diff_of_means, 10_000)\n")))),(0,i.kt)("h1",{id:"compute-and-print-the-p-value-p"},"Compute and print the p-value: p"),(0,i.kt)("p",null,"  p = np.sum(perm_replicates <= nht_diff_obs) / 10000\nprint('p-val =', p)"),(0,i.kt)("h1",{id:"your-p-value-is-00001-which-means-that-only-one-out-of-your-10000-replicates-had-a-result-as-extreme-as-the-actual-difference-between-the-dead-ball-and-live-ball-eras-this-suggests-strong-statistical-significance-"},'"Your p-value is 0.0001, which means that only one out of your 10,000 replicates had a result as extreme as the actual difference between the dead ball and live ball eras. This suggests strong statistical significance. "'),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"* **Hypothesis Testing for correlation**:\n  * Do a permutation test: Permute the x values but leave the y values fixed to generate a new set of (x,y) data. It is exact because it uses all data and eliminates any correlation because which x value pairs to which y value is shuffled.\n  ```\n  # Compute observed correlation: r_obs\n  r_obs = pearson_r(x, y)\n\n  # Initialize permutation replicates: perm_replicates\n  perm_replicates = np.empty(10000)\n\n  # Draw replicates\n  for i in range(10000):\n      # Permute illiteracy measurments: x_permuted\n      x_permuted = np.random.permutation(x)\n\n      # Compute Pearson correlation\n      perm_replicates[i] = pearson_r(x_permuted, y)\n\n  # Compute p-value: p\n  p = np.sum(perm_replicates >= r_obs) / len(perm_replicates)\n  print('p-val =', p)\n  ```\n* **Interpolation:** `np.interp(xcoords_to_interpolate, data_xcoords, data_ycoords, left=None, right=None, period=None)`\n* **Generate uniformly spaced list**: `np.linspace(lower_lim, upper_lim, number_of_items)`\n* **Meshgrid: create a grid by using coordinates of each dimension**: `np.meshgrid(columns, rows)`\n* [See](https://stackoverflow.com/a/42404323)\n\n## Pandas\n* **Import**: `import pandas as pd`\n#### Series (Pandas labelled list)\n* **From list**: `pd.Series(array, ..., index=arr)`\n#### DataFrame (Pandas labelled excel sheets / dictionaries)\nCreation / Conversion\n* **From Dictionary**: ```pd.DataFrame(dict)```\n* **From CSV File**: \n")),(0,i.kt)("p",null,"  dataframe = pd.read_csv(string_filename,\ndelimiter   = str,          # The shorthand 'sep' serves the same purpose\nheader      = line_no_start_of_data,\nindex_col   = i, # Column to use as row labels of the df. Set to False to force pandas not to use the first column as the index, or to a col name if you want to use that col"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"  [chunksize   = n, \n  names       = new_col_labels_list, \n  comment     = str_prefix, \n  parse_dates = [date_col...]\n")),(0,i.kt)("p",null,"  )"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"* Note that dataframes are iterables, so you can use \"next(iterable)\" on them (to get them in the chunksize)\n* **Converting to set**: `set(df['col'])`\n* **add_prefix to all cols**: `df.add_prefix(str)`\n\nAccessing / Selection\n* **For Loop for each row** (note that row is a dataframe / dictionary)\n* `for idx, row in df.iterrows(): ...`\n* Slicing\n* **Get column as Series**: `dataframe[\"column\"]`\n  * **Convert series to numpy.ndarray**: `series.values`\n    * Useful for getting data into right format for sklearn\n* **Extract columns from DF**: `dataframe[[\"column\",...]]`\n  * **Dataframe as rows**: `dataframe[startIdxIncl: endIdxExcl]`\n* Array based selection\n* **Select by label**: \n  * `dataframe.loc[label]` (transposed / series / numpy array)\n  * `dataframe.loc[[rowLbl1, rowLbl2..* .],[colLbl1, colLbl2...]]` (tabular / dataframe)\n  * **Select by index, not name**: Replace loc with iloc\n    * `df.iloc[<slicing for row>, <slicing for column>]`\n  * **Select all rows by column**: Replace label with ':' \n    * `dataframe.loc[<slicing for row>, <slicing for column>]`\n    * `dataframe.loc[:,col]` (Returns numpy array)\n    * `dataframe.loc[:,* [cols]]` (Returns Dataframe)\n  * **MultiIndexes**: `df.loc[(top_lvl_idx,2nd_lvl_idx,...), <slicing for column>]`\n    * e.g. `sales.loc[('NY',1),:], sales.loc[(slice(None),2),:]`\n    * If you have to use `:` for slicing, replace the tuple with `pd.IndexSlice[top_lvl_idx,2nd_lvl_idx,...]`\n      * e.g. `df.loc[pd.IndexSlice[:,2nd_lvl_idx,...], <slicing for column>]`\n      * *Datacamp loves to create the alias `idx=pd.IndexSlice` to shorten the .loc call.*\n    * MultiIndexes: the index is an array instead of a single value (think of nested arrays. e.g. arr[1][2], MultiIndex would be (1,2) or something)\n  * **By Datetime** (if index is datetime): `ts0.loc['2010-August':'2010-10-11 22:00:00']` \n    * *can even be like '2010-Aug'*\n  * (See more: https://www.w3resource.com/pandas/dataframe/dataframe-loc.php)\n    * slicing refers to x[start:end:step].\n    * Special slices:\n      * [:] => Select all\n      * [n:] => from the nth element (inclusive) to the end; note that n starts from 0\n      * [:n] => from the first element to the nth element\n* Conditional Select\n* **Get by bool arr**: `df[bool_arr]`\n* **Get**: `dataframe[dataframe[\"column\"] == condition]`\n* **Assignment:** `df.loc[df['col'] <condition>, 'column_to_set'] = value_to_assign`\n* **Multiple conditional**: `(df['col'] == condition) & (df['col'] == condition)`\n* `dataframe[\"col\"] = dataframe['col'] == condition`\n* **By data type**: `df.select_dtypes(include=[int, float])`\n* **Get data as datetime**: `df['col'].dt`\n* **Format datetime**: `df['col'].dt.strftime('formatstr')`\n  * formatstr: e.g. '%Y' to only get the year\n* **Get data as str**: `df['col'] = df['col'].str`\n* *This can be combined by concatenating string functions behind e.g. .lower(), .strip(), .upper(), .replace(dict), .replace(\"old\",\"new\"), .len()*\n* Other fun obscure string functions include .startswith()\n* Chains must be prefixed with a `.str` in front e.g. `.str.replace('x','').str.replace(...)\n* *Note that these string functions return a df, which can be combined with other str or aggregation functions*\n* **Word count**: `df['col'].str.split().str.len()`\n\nData Addition\n* New Column:\n* **Add new value**: `df.loc[lbl, col] = val` (do this for every row to add the column)\n* **Set column to that value for every row**: `df['col'] = val`\n* **Modify by transformation function**: `df[\"newCol\"] = df[\"oldCol\"].apply(transformFx)`\n  * e.g. `df.apply(lambda row: row.mean(), axis=1)`\n  * Regex can also be used etc\n* **Modify by mapping values to a dictionary**: `df['col'].map(dict_map_vals)`\n* New Row:\n* **Append dataframe**: \n")),(0,i.kt)("p",null,"  df1.append(df2,\nignore_index=False | True # False: Preserve index. True: Number everything from 0 to n\n)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"* **Concatenating / Joining list of series/dataframes**: \n* **Plus** (if indexes are properly set): `df1 + df2`\n* **Concat**\n")),(0,i.kt)("p",null,"  pd.concat(list_of_dataframes, # Concatenating a dictionary will result in the keys becoming the indexes\naxis='index' | 'columns', # 'index': Stack below (\"vertically\"), 'columns': Stack to the right (\"horizontally\")\nkeys=","['one','col','name','per','dataframe','in','list']",",\njoin='inner',     # optional, keep only rows that share common index labels.\nignore_index=bool # If True, prevents repeated integer indices\n)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"* Keys: 1 key per DataFrame in the list, forming the outer index in the MultiIndex. The resulting DataFrame will be something like df['one'][n] to access the first DataFrame, df['col'][n] to access the 2nd DataFrame etc.\n* **Merge / Join**\n")),(0,i.kt)("h1",{id:"inner-join-equivalent-to-pdconcatleftdf-rightdf-columns-joininner"},"Inner join, Equivalent to pd.concat(","[leftDF, rightDF]",", 'columns', join='inner')"),(0,i.kt)("p",null,"  pd.merge(leftDF, rightDF,\non=column_label, # See exposition below\nhow='inner',     # Type of join. See below\nsuffixes=","[sfx_forDF1, sfx_forDF2]"," # If both DFs have columns of same name, add suffix at the end if not merging on those columns\n)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"* **on:** If multiple columns form the identifier: use `on=[col1,col2,...]`. Otherwise, the join will horizontally append copies of the columns even if they are the same.\n* **on:** If df1 and df2 use different labels for the same identifier, use `left_on` and `right_on`.\n* **how:** You can also define various types of joins as specified [here](https://mode.com/sql-tutorial/sql-outer-joins/) \n* See [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html) if need to be more specific\n* **Ordered merge**\n")),(0,i.kt)("h1",{id:"same-as-pdmerge-above-but-designed-for-ordered-data-like-time-series-and-filling-and-interpolation"},"Same as pd.merge above, but designed for ordered data like time series and filling and interpolation."),(0,i.kt)("p",null,'  pd.merge_ordered(leftDF, rightDF,\nfill_method="ffill" # Forward-filling: Replace NaN entries with the most recent non-null entry,'),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"  # Other params same as above)\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"* **Merge with value comparison**\n")),(0,i.kt)("p",null,"  pd.merge_asof(...)"),(0,i.kt)("h1",{id:"pdmerge_asof-is-like-the-pdmerge_ordered-function-it-merges-values-in-order-using-the-on-column"},"pd.merge_asof() is like the pd.merge_ordered() function; it merges values in order using the on column"),(0,i.kt)("h1",{id:"but-for-each-row-in-the-left-dataframe-only-rows-from-the-right-dataframe-whose-on-column-values-are-less-than-the-left-value-will-be-kept"},"but for each row in the left DataFrame, only rows from the right DataFrame whose 'on' column values are less than the left value will be kept."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Data Deletion\n* **Drop rows / columns**: \n")),(0,i.kt)("p",null,"df.drop(labels,               # Index or column labels to drop.\naxis= 'index' | 'columns'\n)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"* **Drop rows / columns with missing values**: \n")),(0,i.kt)("p",null,"df.dropna(\nsubset = ","[column_labels]","    # Labels along other axis to consider, e.g. if you are dropping rows these would be a list of columns to include.\nhow = 'any' | 'all'         # 'any' : If any NA values are present, drop that row or column. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"                            # 'all' : If all values are NA, drop that row or column.\n\nthresh = n                  # Drop unless there are at least n non-NA values along that axis\naxis = 'index' | 'columns'  # 0/\u2018index\u2019   : Drop rows    which contain missing values. \n                            # 1/\u2018columns\u2019 : Drop columns which contain missing values.\n")),(0,i.kt)("p",null,")"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"  * See [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)\n* **Drop duplicate rows**: \n")),(0,i.kt)("p",null,"df.drop_duplicates(\nsubset = arr_of_col_names,\nkeep = 'first' | 'last' | False,\ninplace = bool\n)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"\nValue Modification\n* **Dividing one DF by another**:\n")),(0,i.kt)("p",null,"  DF1.divide(DF2,\naxis='rows'/'columns' # Divide the DF1 by DF2 along each row\n)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"\nMetadata modification\n* **Re-labelling columns**: `df.columns = arr_of_labels`\n\nDatatype modification\n* **Convert to datetime**: \n")),(0,i.kt)("p",null,"pd.to_datetime(df","['col']",",\nformat = date_str_format,\ninfer_datetime_format = True | False,  # Infer format based on first non-NaN element. Can increase parsing speed by 5-10x (disabled by default)\nerrors = 'raise' | 'coerce' | 'ignore' # raise = raise exception, coerce = set to NaT (not a time), ignore = ignore\n)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"* A datetime column will allow you to manipulate the datetime directly & search for rows that match the date using df.loc\n  * You can also extract information from it using its attributes. See any attr under [pandas.Series.dt](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.month.html).\n    * **Adding a new column for min/year etc**: `df[\"month\"] = df[\"date\"].apply(lambda row: row.month)`\n* Example of merging 2 string cols into 1 datetime: `times_tz_none = pd.to_datetime(la['Date (MM/DD/YYYY)'] + ' ' + la['Wheels-off Time'])`\n* **Convert to numeric**: `pd.to_numeric(df['col'], errors='coerce')`\n* **Convert to categorical**: \n")),(0,i.kt)("p",null,"df","['col']"," = pd.Categorical(values=df","['col']",", categories=arr_of_values,\nordered=True # True: ordered categoricals\n)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"* **Bin values into discrete intervals (Convert numbers to categories)**: \n")),(0,i.kt)("p",null,"df","['col']"," = pd.cut(df","['col']",",\nbins = arr_of_bin_edges,    # e.g. ","[0, 60, 180, np.inf]","\nlabels = arr_of_categories  # e.g. ","['short', 'medium', 'long']","\n)"),(0,i.kt)("h1",{id:"alternatively-if-you-just-want-to-cut-them-into-equal-sized-bins"},"Alternatively, if you just want to cut them into equal sized bins"),(0,i.kt)("p",null,"df","['col']"," = pd.cut(df","['col']",",\nbins = bin_counts\n)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"* **Convert to other types**: `df['newCol'] = df['dataCol'].astype('type')`\n\nDatetime timezone modification\n* **Localizing timezone**: `series.dt.tz_localize(\"US/Central\")`\n* **Converting timezone**: `series.dt.tz_convert(\"US/Central\")`\n\nOperations that involve boolean arrays:\n* Operations that convert DF into boolean DF:\n  * **Value meets condition**: `df_bool_arr = df['col'] > val`\n  * **Value membership**: `df['col'].isin(arr_of_acceptable_values)`\n  * **Value is null or NA**: `df.isnull()` or `df.isna()` (they are the same)\n  * **Value is not null**: `df.notnull()` or `df.notna()`\n  * **Row is duplicated**: `df.duplicated(subset = arr_of_col_names, keep = 'first' | 'last' | False)`\n* **Negate boolean array**: `~bool_arr`\n* **Subsetting with bool array**: `df[bool_df]`\n\nUnary Operations\n* **Compute % change from the immediate previous**: `series.pct_change(offset=1)`: \n  * Row by default. Useful in comparing the percentage of change in a time series of elements. \n  * Offset is in unit time specified in sample\n\nGrouping Data\n* **Downsample time series**: `df['col'].resample(time_str_format).agg_fx()`\n  * e.g. `df.Temperature.resample('6h').mean()` = group an hourly based time series into averaged quarterly data\n  * `df.resample('A').mean()`: resample w/ annual frequency, assumes index is a datetime\n  * `df.resample('A', on=column_label).mean()`: resample w.r.t a column label that isn't the index\n* `groupby` function:\n  * **Group-by (Single index)**: `df.groupby('idx')`\n  * **Group-by (Multi-Index)**: `df.groupby([indexes])`\n  * **Group-by (Rows)**: `df.groupby(pd.Series(['row_vals'])`\n  * Note that the groupby function should be followed up with a column + aggregate for it to be useful, unless you want to literally count the number of rows etc\n    * e.g. count_by_class = by_class['survived'].count()\n  * Group by Day example: `by_day = sales.groupby(sales.index.strftime('%a'))`\n  * **Multiple Aggregation (columns)**: `sales.groupby('city')[['bread','butter']].max()`\n    * ![](../../static/img/groupby-max.jpg)\n  * **Multiple aggregation (functions)**: `sales.groupby('city')[['bread','butter']].agg(['max','sum'])`\n    * ![](../../static/img/groupby-max-sum.jpg)\n    * **Custom aggregation (own function)**: You can define a function that accepts a Series and returns a single value.\n    * **Separate aggregation per column (dictionary)**: You can define a dictionary and put it into .agg; the key is the column, the value is the aggregation function (e.g. max, min)\n    * `df.groupby(...).transform(fx)`: Transform after aggregation (group by, then transform values based on their groups)\n      * Output is the same shape as before groupby.\n      * e.g. `def zscore(series): return (series - series.mean()) / series.std()\n      * Usage: `df.groupby('x')['y'].transform(zscore)`\n* **Filtering (after groupby)**:\n  * `by_company = sales.groupby(\"Company\")`\n  * **Compute sum of 'Units'**: `by_com_sum = sales['Units'].sum()`\n  * **Filter 'Units' where sum > 35**: `by_com_filt = by_company.filter(lambda g:g['Units'].sum() > 35)`\n\nAggregation Operations\n* **Get Uniques**: `series.unique()`\n* **Aggregate duplicates**: `df.groupby(by = arr_of_col_names).agg(col_to_fx_dict).reset_index()`\n  * *col_to_fx_dict*: e.g. {'height: 'max', 'weight': mean}\n* **sum**: `df['col'].sum(axis={index (0), columns (1)})`\n  * *You can also sum booleans to count number of True values*\n  * **Count number of missing values by col**: `df.isna().sum()`\n* **mean**: `df['col'].mean()`\n* **max**: `df['col'].max()`\n* **argmax** (idx of max val): `df['col'].argmax()`\n* **count**: `df['col'].count()`\n* **quantile**: `quantile([start, end (0-1)])`\n  * Get single value: `quantile(pct_from_0_to_1)`\n* **std.dev**: `std()`\n* **rolling mean**: `df['col'].rolling(window=numRows).mean()`\n* **# of uniques:** `nunique()`\n* **Count number of times value appeared**: `value_counts()` / `series.value_counts()`\n  * Returns dataframe (dictionary)\nSorting Operations\n* **Sort by current index**: `df.sort_index(level=idx_lvl)`\n  * You may want to change the index by using `df.set_index()` first\n* **Sort values by column name**: `df.sort_values(by = arr_of_col_names)` or `df.sort_values('col')`\n  * **Sort df chosen by boolean array**: `df[bool_arr].sort_values(...)`\n\nWindowing Operations\n* [Documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html)\n* `df.expanding()`\n  * See [this](https://stackoverflow.com/questions/45370666/what-are-pandas-expanding-window-functions): \"A common alternative to rolling statistics is to use an expanding window, which yields the value of the statistic with all the data available up to that point in time\"\n\nOperations on Indexes:\n* **Interpolate**: `ts2_interp = ts2.reindex(ts1.index).interpolate(how='linear') `\n  * in the above example, the index is changed to datetime. ts1 contains all datetime, ts2 has some missing data\n* **Changing metadata / restructuring**\n  * **Reindex** (Change values of the 1st column): `df = df.reindex(col/df2.index,[method=pad/backfill/nearest])`\n    * *Conform DataFrame to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False*\n  * **Change index entirely**: `df.set_index('colname',inplace=bool)`. \n    * This is usually done as an interim operation to make naivgating the DF easier, or for using `sort_index()`\n    * [Reindex vs set_index](https://stackoverflow.com/questions/50741330/difference-between-df-reindex-and-df-set-index-methods-in-pandas)\n  * **Reset index**: `df.reset_index()`\n\nOperations on Restructuring Data (Pivoting)\n* **Pivot (reorder data by changing the index, columns & values. REQUIRES UNIQUE INDEX)**: \n")),(0,i.kt)("p",null,"df.pivot(\nindex=new_row_index,      # Each unique value in the column is now a primary key of the row. Aggfunc aggregates if there are duplicate PKs.\ncolumns=new_columns,      # Each unique value in the column is now a column\nvalues=old_cols_to_vals   # Each value in the column are now assigned to row-column where they occur. Aggregate if needed.\n)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"* **Pivot Table: Same as pivot, but deal with duplicate index values with a reduction using aggfunc**:\n")),(0,i.kt)("p",null,"df.pivot_table(\nindex=new_row_index,        # Each unique value in the column is now a primary key of the row. Aggfunc aggregates if there are duplicate PKs.\ncolumns=new_columns,        # Each unique value in the column is now a column\nvalues=old_cols_to_vals     # Each value in the column are now assigned to row-column where they occur. Aggregate if needed.\naggfunc=fx/'predefined_fx', # Aggregate duplicate index values using this function\nmargins=bool                # If True, add a \"All\" row at the bottom which aggregates all data\n)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"* **Melt: undoing a pivot**: \n")),(0,i.kt)("p",null,"pd.melt(dataframe,\nid_vars=","['cols']",",               # column names to keep as columns\nvalue_vars=","['cols']",",            # column names to convert into key-value pairs, under two columns: "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"                                # 1st column specified as \"variable\" which uses the original column name\nvalue_name=['value_col_names'], # 2nd column whose name is specified by value_name\ncol_level = 0                   # use col_level = 0 to convert it into purely variable-value pair, removing any id_vars / indexes currently in use\n")),(0,i.kt)("p",null,")"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"  * [documentation](https://pandas.pydata.org/docs/reference/api/pandas.melt.html)\n* **Stack**: `df.stack(level='col')`\n  * [stack the prescribed level(s) by shifting columns to index](https://www.w3resource.com/pandas/dataframe/dataframe-stack.php)\n* **Unstacking**: `df.unstack(level='col'/num)`\n  * [form new level of columns whose inner-most level consists of the pivoted index labels](https://www.w3resource.com/pandas/dataframe/dataframe-unstack.php)\n* **Swap level**: [swap ordering of stacked levels](https://www.geeksforgeeks.org/python-pandas-multiindex-swaplevel/)\n")),(0,i.kt)("p",null,"dataframe.swaplevel(\nindex1_level, # e.g. 0 for the outer-most level\nindex2_level, # e.g. 1 for the inner level\naxis='index'|'columns')"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"*  Operations involving missing values and cleaning data:\n* **Impute/Replace missing vals**: `df.fillna({'col' : val_arr}, inplace=True)`\n  * See [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html?highlight=fillna#pandas.DataFrame.fillna)\n\nOperations involving visualization \n* DataFrame Info\n  * **Correlation between columns**: `df.corr()`\n    * Correlation of A and B = Correlation of B and A\n  * **Display head**: `df.head(<rows=5>)`\n  * **Display tail**: `df.tail(<rows=5>)`\n  * **Display schema**: `df.info()` (Shows col name, non-null entries & datatype)\n    * **Display columns**: `df.columns`\n  * **Display summary stats (e.g. std,min,max,quartiles)**: `df.describe()` (also works on columns of the df)\n  * **Display datatype**: `df.dtype` or `df.dtypes` (works on columns of the df)\n  * **Size / Length / Shape**: `df.shape`\n* Plotting data from DataFrame (See matplotlib.pyplot):\n  * `dataframe.plot(kind='scatter', x='col1', y='col2', [color='str', s=size_value,subplots=bool])`\n    * you can plot all data by omitting x and y\n    * subplots: plot in separate graphs\n  * `dataframe.boxplot(column = [y_axis_col_values], by=[x_axis_col_values])`\n  * `ax = df[list_of_columns].plot() `\n    * You can customize the plot by calling the functions below on ax:\n      * `ax.set_ylabel(\"% Change of Host Country Medal Count\")`\n      * `ax.set_title(\"Is there a Host Country Advantage?\")`\n      * `ax.set_xticklabels(editions['City'])`\n    * plot all of the columns (their x and y) on the same graph with their own colours\n    * plt.title(str)\n    * plt.xlabel(str)\n    * plt.ylabel(str)\n    * plt.show()\n  * Subplots\n    * `fig, axes = plt.subplots(nrows=num_of_rows, ncols=num_of_columns)`\n    * `df.plot(ax=axes[0], kind='hist', normed=True, bins=30, range=(0,.3))`\n    * `df.fraction.plot(ax=axes[1], kind='hist', normed=True, cumulative=True, bins=30, range=(0,.3))`\n      * `kind='bar'`\n")))}u.isMDXComponent=!0},5567:function(e,a,t){a.Z=t.p+"assets/images/covariance-66ba702f083d4e255fbf35141322a4cc.jpg"},4694:function(e,a,t){a.Z=t.p+"assets/images/pca-36cea45e68e6bdc68fd9f5be6c2d29bc.jpg"}}]);